{
  "01-ai/Yi-1.5-34B-Chat-16K": {
    "description": "Yi-1.5 34B, с богатым набором обучающих образцов, демонстрирует превосходные результаты в отраслевых приложениях."
  },
  "01-ai/Yi-1.5-6B-Chat": {
    "description": "Yi-1.5-6B-Chat — это вариант серии Yi-1.5, относящийся к открытым моделям для чата. Yi-1.5 является обновленной версией Yi, которая была непрерывно предобучена на 500B высококачественных корпусах и дообучена на более чем 3M разнообразных образцах. По сравнению с Yi, Yi-1.5 демонстрирует более сильные способности в кодировании, математике, выводах и соблюдении инструкций, сохраняя при этом отличные навыки понимания языка, логического вывода и понимания прочитанного. Эта модель имеет версии с длиной контекста 4K, 16K и 32K, с общим объемом предобучения 3.6T токенов."
  },
  "01-ai/Yi-1.5-9B-Chat-16K": {
    "description": "Yi-1.5 9B поддерживает 16K токенов, обеспечивая эффективные и плавные возможности генерации языка."
  },
  "360gpt-pro": {
    "description": "360GPT Pro, как важный член серии моделей AI от 360, удовлетворяет разнообразные приложения обработки текста с высокой эффективностью, поддерживает понимание длинных текстов и многораундные диалоги."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo предлагает мощные вычислительные и диалоговые возможности, обладает выдающимся пониманием семантики и эффективностью генерации, что делает его идеальным решением для интеллектуальных помощников для предприятий и разработчиков."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K акцентирует внимание на семантической безопасности и ответственности, специально разработан для приложений с высокими требованиями к безопасности контента, обеспечивая точность и надежность пользовательского опыта."
  },
  "360gpt2-o1": {
    "description": "360gpt2-o1 использует дерево поиска для построения цепочек размышлений и вводит механизм рефлексии, обучаясь с помощью усиленного обучения, модель обладает способностью к саморефлексии и исправлению ошибок."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro — это продвинутая модель обработки естественного языка, выпущенная компанией 360, обладающая выдающимися способностями к генерации и пониманию текста, особенно в области генерации и творчества, способная обрабатывать сложные языковые преобразования и ролевые задачи."
  },
  "360zhinao2-o1": {
    "description": "Модель 360zhinao2-o1 использует дерево поиска для построения цепочки размышлений и включает механизм рефлексии, обучаясь с помощью усиленного обучения, что позволяет модели самостоятельно рефлексировать и исправлять ошибки."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra — это самая мощная версия в серии больших моделей Xinghuo, которая, обновив сетевые поисковые связи, улучшает понимание и обобщение текстового контента. Это всестороннее решение для повышения производительности в офисе и точного реагирования на запросы, являющееся ведущим интеллектуальным продуктом в отрасли."
  },
  "Baichuan2-Turbo": {
    "description": "Использует технологии улучшенного поиска для полной связи между большой моделью и отраслевыми знаниями, а также знаниями из сети. Поддерживает загрузку различных документов, таких как PDF и Word, а также ввод URL, обеспечивая своевременное и полное получение информации с точными и профессиональными результатами."
  },
  "Baichuan3-Turbo": {
    "description": "Оптимизирован для высокочастотных корпоративных сценариев, значительно улучшает результаты и предлагает высокую стоимость. По сравнению с моделью Baichuan2, создание контента увеличилось на 20%, ответы на вопросы на 17%, а способности ролевого взаимодействия на 40%. Общая эффективность лучше, чем у GPT3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Обладает 128K сверхдлинным контекстным окном, оптимизированным для высокочастотных корпоративных сценариев, значительно улучшает результаты и предлагает высокую стоимость. По сравнению с моделью Baichuan2, создание контента увеличилось на 20%, ответы на вопросы на 17%, а способности ролевого взаимодействия на 40%. Общая эффективность лучше, чем у GPT3.5."
  },
  "Baichuan4": {
    "description": "Модель обладает лучшими возможностями в стране, превосходя зарубежные модели в задачах на знание, длинные тексты и генерацию контента. Также обладает передовыми мультимодальными возможностями и показывает отличные результаты в нескольких авторитетных тестах."
  },
  "Baichuan4-Air": {
    "description": "Модель обладает лучшими в стране возможностями, превосходя зарубежные модели в задачах на китайском языке, таких как энциклопедические знания, длинные тексты и генерация контента. Также обладает передовыми мультимодальными возможностями и демонстрирует отличные результаты в нескольких авторитетных оценочных тестах."
  },
  "Baichuan4-Turbo": {
    "description": "Модель обладает лучшими в стране возможностями, превосходя зарубежные модели в задачах на китайском языке, таких как энциклопедические знания, длинные тексты и генерация контента. Также обладает передовыми мультимодальными возможностями и демонстрирует отличные результаты в нескольких авторитетных оценочных тестах."
  },
  "DeepSeek-R1": {
    "description": "Современная эффективная LLM, специализирующаяся на логическом выводе, математике и программировании."
  },
  "DeepSeek-R1-Distill-Llama-70B": {
    "description": "DeepSeek R1 — более крупная и умная модель в наборе DeepSeek, была дистиллирована в архитектуру Llama 70B. На основе бенчмарков и человеческой оценки эта модель более умная, чем оригинальная Llama 70B, особенно в задачах, требующих математической и фактической точности."
  },
  "DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Модель DeepSeek-R1, основанная на Qwen2.5-Math-1.5B, оптимизирует производительность вывода с помощью усиленного обучения и данных холодного старта, обновляя стандарт многозадачности в открытых моделях."
  },
  "DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Модель DeepSeek-R1, основанная на Qwen2.5-14B, оптимизирует производительность вывода с помощью усиленного обучения и данных холодного старта, обновляя стандарт многозадачности в открытых моделях."
  },
  "DeepSeek-R1-Distill-Qwen-32B": {
    "description": "Серия DeepSeek-R1 оптимизирует производительность вывода с помощью усиленного обучения и данных холодного старта, обновляя стандарт многозадачности в открытых моделях, превосходя уровень OpenAI-o1-mini."
  },
  "DeepSeek-R1-Distill-Qwen-7B": {
    "description": "Модель DeepSeek-R1, основанная на Qwen2.5-Math-7B, оптимизирует производительность вывода с помощью усиленного обучения и данных холодного старта, обновляя стандарт многозадачности в открытых моделях."
  },
  "Doubao-1.5-vision-pro-32k": {
    "description": "Doubao-1.5-vision-pro - совершенно обновленная многомодальная большая модель, поддерживающая распознавание изображений с любым разрешением и экстремальными соотношениями сторон, улучшенная способность визуального вывода, распознавания документов, понимания деталей и соблюдения инструкций."
  },
  "Doubao-lite-128k": {
    "description": "Doubao-lite обеспечивает выдающуюся скорость отклика и лучшее соотношение цены и качества, предлагая клиентам больше гибкости в различных сценариях. Поддерживает вывод и настройку с 128k контекстным окном."
  },
  "Doubao-lite-32k": {
    "description": "Doubao-lite обеспечивает выдающуюся скорость отклика и лучшее соотношение цены и качества, предлагая клиентам больше гибкости в различных сценариях. Поддерживает вывод и настройку с 32k контекстным окном."
  },
  "Doubao-lite-4k": {
    "description": "Doubao-lite обеспечивает выдающуюся скорость отклика и лучшее соотношение цены и качества, предлагая клиентам больше гибкости в различных сценариях. Поддерживает вывод и настройку с 4k контекстным окном."
  },
  "Doubao-pro-128k": {
    "description": "Модель основных характеристик с лучшими показателями, подходит для обработки сложных задач. Хорошо справляется с задачами референсного ответа, резюмирования, творчества, классификации текста, ролевого взаимодействия и т.д. Поддерживает вывод и настройку с 128k контекстным окном."
  },
  "Doubao-pro-256k": {
    "description": "Лучшая модель для основных задач, подходит для обработки сложных задач, демонстрирует отличные результаты в таких сценариях, как ответ на вопросы, резюмирование, творчество, классификация текста и ролевые игры. Поддерживает вывод на 256k контекстных окнах и тонкую настройку."
  },
  "Doubao-pro-32k": {
    "description": "Модель основных характеристик с лучшими показателями, подходит для обработки сложных задач. Хорошо справляется с задачами референсного ответа, резюмирования, творчества, классификации текста, ролевого взаимодействия и т.д. Поддерживает вывод и настройку с 32k контекстным окном."
  },
  "Doubao-pro-4k": {
    "description": "Модель основных характеристик с лучшими показателями, подходит для обработки сложных задач. Хорошо справляется с задачами референсного ответа, резюмирования, творчества, классификации текста, ролевого взаимодействия и т.д. Поддерживает вывод и настройку с 4k контекстным окном."
  },
  "Doubao-vision-lite-32k": {
    "description": "Модель Doubao-vision - это многомодальная большая модель, представленная Doubao, обладающая мощными способностями понимания и вывода изображений, а также точным пониманием инструкций. Модель демонстрирует выдающуюся производительность в извлечении текстовой информации из изображений и задачах вывода на основе изображений, что позволяет применять ее в более сложных и широких задачах визуального вопроса и ответа."
  },
  "Doubao-vision-pro-32k": {
    "description": "Модель Doubao-vision - это многомодальная большая модель, представленная Doubao, обладающая мощными способностями понимания и вывода изображений, а также точным пониманием инструкций. Модель демонстрирует выдающуюся производительность в извлечении текстовой информации из изображений и задачах вывода на основе изображений, что позволяет применять ее в более сложных и широких задачах визуального вопроса и ответа."
  },
  "ERNIE-3.5-128K": {
    "description": "Флагманская крупномасштабная языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными возможностями, способная удовлетворить большинство требований к диалоговым ответам, генерации контента и сценариям использования плагинов; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-3.5-8K": {
    "description": "Флагманская крупномасштабная языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными возможностями, способная удовлетворить большинство требований к диалоговым ответам, генерации контента и сценариям использования плагинов; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-3.5-8K-Preview": {
    "description": "Флагманская крупномасштабная языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными возможностями, способная удовлетворить большинство требований к диалоговым ответам, генерации контента и сценариям использования плагинов; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-4.0-8K-Latest": {
    "description": "Флагманская сверхкрупномасштабная языковая модель, разработанная Baidu, которая по сравнению с ERNIE 3.5 обеспечивает полное обновление возможностей модели и широко применяется в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-4.0-8K-Preview": {
    "description": "Флагманская сверхкрупномасштабная языковая модель, разработанная Baidu, которая по сравнению с ERNIE 3.5 обеспечивает полное обновление возможностей модели и широко применяется в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах."
  },
  "ERNIE-4.0-Turbo-8K-Latest": {
    "description": "Флагманская 超大型 языковая модель, разработанная Baidu, демонстрирует отличные результаты и хорошо подходит для сложных задач в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая своевременность ответов. По сравнению с ERNIE 4.0 имеет лучшие показатели производительности."
  },
  "ERNIE-4.0-Turbo-8K-Preview": {
    "description": "Флагманская сверхкрупномасштабная языковая модель, разработанная Baidu, демонстрирующая отличные результаты в комплексной эффективности, широко применяемая в сложных задачах в различных областях; поддерживает автоматическую интеграцию с плагином поиска Baidu, обеспечивая актуальность информации в ответах. По сравнению с ERNIE 4.0, она демонстрирует лучшие показатели производительности."
  },
  "ERNIE-Character-8K": {
    "description": "Специализированная языковая модель, разработанная Baidu для вертикальных сценариев, подходящая для применения в играх (NPC), диалогах службы поддержки, ролевых играх и других сценариях, обладающая ярко выраженным и согласованным стилем персонажей, высокой способностью следовать инструкциям и отличной производительностью вывода."
  },
  "ERNIE-Lite-Pro-128K": {
    "description": "Легковесная языковая модель, разработанная Baidu, которая сочетает в себе отличные результаты модели и производительность вывода, превосходя ERNIE Lite, подходит для использования в системах с низкой вычислительной мощностью."
  },
  "ERNIE-Speed-128K": {
    "description": "Новая высокопроизводительная языковая модель, разработанная Baidu в 2024 году, обладающая выдающимися универсальными возможностями, подходит для использования в качестве базовой модели для тонкой настройки, лучше справляясь с задачами в специфических сценариях, при этом обладая отличной производительностью вывода."
  },
  "ERNIE-Speed-Pro-128K": {
    "description": "Новая высокопроизводительная языковая модель, разработанная Baidu в 2024 году, обладающая выдающимися универсальными возможностями, превосходящая ERNIE Speed, подходит для использования в качестве базовой модели для тонкой настройки, лучше справляясь с задачами в специфических сценариях, при этом обладая отличной производительностью вывода."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) — это инновационная модель, подходящая для многообластных приложений и сложных задач."
  },
  "InternVL2-8B": {
    "description": "InternVL2-8B — это мощная визуально-языковая модель, поддерживающая многомодальную обработку изображений и текста, способная точно распознавать содержимое изображений и генерировать соответствующие описания или ответы."
  },
  "InternVL2.5-26B": {
    "description": "InternVL2.5-26B — это мощная визуально-языковая модель, поддерживающая многомодальную обработку изображений и текста, способная точно распознавать содержимое изображений и генерировать соответствующие описания или ответы."
  },
  "Llama-3.2-11B-Vision-Instruct": {
    "description": "Отличные способности к визуальному выводу на изображениях высокого разрешения, подходящие для приложений визуального понимания."
  },
  "Llama-3.2-90B-Vision-Instruct\t": {
    "description": "Передовые способности к визуальному выводу, подходящие для приложений визуального понимания."
  },
  "LoRA/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 72B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "LoRA/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 7B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "Meta-Llama-3.1-405B-Instruct": {
    "description": "Текстовая модель Llama 3.1 с оптимизацией под инструкции, разработанная для многоязычных диалоговых случаев, показывает отличные результаты по сравнению с многими доступными открытыми и закрытыми чат-моделями на общепринятых отраслевых бенчмарках."
  },
  "Meta-Llama-3.1-70B-Instruct": {
    "description": "Текстовая модель Llama 3.1 с оптимизацией под инструкции, разработанная для многоязычных диалоговых случаев, показывает отличные результаты по сравнению с многими доступными открытыми и закрытыми чат-моделями на общепринятых отраслевых бенчмарках."
  },
  "Meta-Llama-3.1-8B-Instruct": {
    "description": "Текстовая модель Llama 3.1 с оптимизацией под инструкции, разработанная для многоязычных диалоговых случаев, показывает отличные результаты по сравнению с многими доступными открытыми и закрытыми чат-моделями на общепринятых отраслевых бенчмарках."
  },
  "Meta-Llama-3.2-1B-Instruct": {
    "description": "Современная передовая компактная языковая модель с выдающимися способностями к пониманию языка, логическому выводу и генерации текста."
  },
  "Meta-Llama-3.2-3B-Instruct": {
    "description": "Современная передовая компактная языковая модель с выдающимися способностями к пониманию языка, логическому выводу и генерации текста."
  },
  "Meta-Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 — это самая современная многоязычная открытая языковая модель из серии Llama, которая позволяет получить производительность, сопоставимую с 405B моделями, по крайне низкой цене. Основана на структуре Transformer и улучшена с помощью контролируемой донастройки (SFT) и обучения с подкреплением на основе человеческой обратной связи (RLHF) для повышения полезности и безопасности. Ее версия с оптимизацией под инструкции специально разработана для многоязычных диалогов и показывает лучшие результаты по сравнению с многими открытыми и закрытыми чат-моделями на нескольких отраслевых бенчмарках. Дата окончания знаний — декабрь 2023 года."
  },
  "MiniMax-Text-01": {
    "description": "В серии моделей MiniMax-01 мы сделали смелые инновации: впервые в крупномасштабном масштабе реализован линейный механизм внимания, традиционная архитектура Transformer больше не является единственным выбором. Объем параметров этой модели достигает 456 миллиардов, из которых 45,9 миллиарда активируются за один раз. Комплексная производительность модели сопоставима с ведущими зарубежными моделями, при этом она может эффективно обрабатывать контекст длиной до 4 миллионов токенов, что в 32 раза больше, чем у GPT-4o, и в 20 раз больше, чем у Claude-3.5-Sonnet."
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) — это высокоточная модель команд, подходящая для сложных вычислений."
  },
  "OpenGVLab/InternVL2-26B": {
    "description": "InternVL2 демонстрирует превосходные результаты в различных визуально-языковых задачах, включая понимание документов и графиков, понимание текстов сцены, OCR, решение научных и математических задач."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Та же модель Phi-3-medium, но с большим размером контекста для RAG или нескольких подсказок."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Модель с 14B параметрами, демонстрирующая лучшее качество, чем Phi-3-mini, с акцентом на высококачественные, насыщенные рассуждениями данные."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Та же модель Phi-3-mini, но с большим размером контекста для RAG или нескольких подсказок."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "Самая маленькая модель в семействе Phi-3. Оптимизирована как для качества, так и для низкой задержки."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Та же модель Phi-3-small, но с большим размером контекста для RAG или нескольких подсказок."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Модель с 7B параметрами, демонстрирующая лучшее качество, чем Phi-3-mini, с акцентом на высококачественные, насыщенные рассуждениями данные."
  },
  "Phi-3.5-mini-instruct": {
    "description": "Обновленная версия модели Phi-3-mini."
  },
  "Phi-3.5-vision-instrust": {
    "description": "Обновленная версия модели Phi-3-vision."
  },
  "Pro/OpenGVLab/InternVL2-8B": {
    "description": "InternVL2 демонстрирует превосходные результаты в различных визуально-языковых задачах, включая понимание документов и графиков, понимание текстов сцены, OCR, решение научных и математических задач."
  },
  "Pro/Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 1.5B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей. По сравнению с Qwen1.5-1.8B-Chat, Qwen2-1.5B-Instruct демонстрирует значительное улучшение производительности в тестах MMLU, HumanEval, GSM8K, C-Eval и IFEval, несмотря на немного меньшее количество параметров."
  },
  "Pro/Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-7B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 7B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она может обрабатывать большие объемы входных данных. Эта модель показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей и демонстрируя конкурентоспособность с проприетарными моделями в некоторых задачах. Qwen2-7B-Instruct показывает значительное улучшение производительности в нескольких оценках по сравнению с Qwen1.5-7B-Chat."
  },
  "Pro/Qwen/Qwen2-VL-7B-Instruct": {
    "description": "Qwen2-VL - это последняя версия модели Qwen-VL, которая достигла передовых результатов в тестировании визуального понимания."
  },
  "Pro/Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 7B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct — это последняя версия серии языковых моделей, специфичных для кода, выпущенная Alibaba Cloud. Эта модель значительно улучшила способности генерации кода, вывода и исправления на основе Qwen2.5, обучаясь на 5.5 триллионах токенов. Она не только усилила кодирование, но и сохранила преимущества в математике и общих способностях. Модель предоставляет более полную основу для практических приложений, таких как интеллектуальные агенты кода."
  },
  "Pro/THUDM/glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat — это открытая версия предобученной модели из серии GLM-4, выпущенная Zhizhu AI. Эта модель показывает отличные результаты в семантике, математике, выводах, коде и знаниях. Кроме поддержки многократных диалогов, GLM-4-9B-Chat также обладает продвинутыми функциями, такими как веб-браузинг, выполнение кода, вызов пользовательских инструментов (Function Call) и вывод длинных текстов. Модель поддерживает 26 языков, включая китайский, английский, японский, корейский и немецкий. В нескольких бенчмарках GLM-4-9B-Chat демонстрирует отличные результаты, такие как AlignBench-v2, MT-Bench, MMLU и C-Eval. Эта модель поддерживает максимальную длину контекста 128K и подходит для академических исследований и коммерческих приложений."
  },
  "Pro/deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 — это модель вывода, управляемая обучением с подкреплением (RL), которая решает проблемы повторяемости и читаемости в модели. Перед RL DeepSeek-R1 вводит данные холодного старта, что дополнительно оптимизирует производительность вывода. Она показывает сопоставимые результаты с OpenAI-o1 в математических, кодовых и задачах вывода и улучшает общую эффективность благодаря тщательно продуманным методам обучения."
  },
  "Pro/deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 — это языковая модель с 6710 миллиардами параметров, использующая архитектуру смешанных экспертов (MoE) и многофункциональное внимание (MLA), в сочетании с стратегией балансировки нагрузки без вспомогательных потерь, оптимизирующая эффективность вывода и обучения. После предобучения на 14.8 триллионах высококачественных токенов и последующей контролируемой донастройки и обучения с подкреплением, DeepSeek-V3 превосходит другие открытые модели и приближается к ведущим закрытым моделям."
  },
  "Pro/google/gemma-2-9b-it": {
    "description": "Gemma — это одна из легковесных, передовых открытых моделей, разработанных Google. Это крупная языковая модель с только декодером, поддерживающая английский язык, предлагающая открытые веса, предобученные варианты и варианты с дообучением на инструкциях. Модель Gemma подходит для различных задач генерации текста, включая вопросы и ответы, резюме и выводы. Эта 9B модель была обучена на 8 триллионах токенов. Ее относительно небольшой размер позволяет развертывать ее в условиях ограниченных ресурсов, таких как ноутбуки, настольные компьютеры или ваша собственная облачная инфраструктура, что позволяет большему количеству людей получить доступ к передовым моделям ИИ и способствовать инновациям."
  },
  "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "Meta Llama 3.1 — это семейство многоязычных крупных языковых моделей, разработанных Meta, включая предобученные и дообученные на инструкциях варианты с параметрами 8B, 70B и 405B. Эта 8B модель с дообучением на инструкциях оптимизирована для многоязычных диалоговых сценариев и показывает отличные результаты в нескольких отраслевых бенчмарках. Обучение модели использовало более 150 триллионов токенов открытых данных и применяло такие технологии, как контролируемое дообучение и обучение с подкреплением на основе человеческой обратной связи для повышения полезности и безопасности модели. Llama 3.1 поддерживает генерацию текста и кода, с датой окончания знаний в декабре 2023 года."
  },
  "QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview — это инновационная модель обработки естественного языка, способная эффективно обрабатывать сложные задачи генерации диалогов и понимания контекста."
  },
  "Qwen/QVQ-72B-Preview": {
    "description": "QVQ-72B-Preview — это исследовательская модель, разработанная командой Qwen, сосредоточенная на способностях визуального вывода, обладающая уникальными преимуществами в понимании сложных сцен и решении визуально связанных математических задач."
  },
  "Qwen/QwQ-32B-Preview": {
    "description": "QwQ-32B-Preview — это последняя экспериментальная исследовательская модель Qwen, сосредоточенная на повышении возможностей вывода ИИ. Исследуя сложные механизмы, такие как смешение языков и рекурсивные выводы, основные преимущества включают мощные аналитические способности, математические и программные навыки. В то же время существуют проблемы с переключением языков, циклом вывода, соображениями безопасности и различиями в других способностях."
  },
  "Qwen/Qwen2-1.5B-Instruct": {
    "description": "Qwen2-1.5B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 1.5B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей. По сравнению с Qwen1.5-1.8B-Chat, Qwen2-1.5B-Instruct демонстрирует значительное улучшение производительности в тестах MMLU, HumanEval, GSM8K, C-Eval и IFEval, несмотря на немного меньшее количество параметров."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 — это передовая универсальная языковая модель, поддерживающая множество типов команд."
  },
  "Qwen/Qwen2-7B-Instruct": {
    "description": "Qwen2-72B-Instruct — это языковая модель с дообучением на инструкциях в серии Qwen2, с параметрами 72B. Эта модель основана на архитектуре Transformer и использует такие технологии, как активационная функция SwiGLU, смещение внимания QKV и групповой запрос внимания. Она может обрабатывать большие объемы входных данных. Эта модель показывает отличные результаты в понимании языка, генерации, многоязычных способностях, кодировании, математике и выводах в различных бенчмарках, превосходя большинство открытых моделей и демонстрируя конкурентоспособность с проприетарными моделями в некоторых задачах."
  },
  "Qwen/Qwen2-VL-72B-Instruct": {
    "description": "Qwen2-VL - это последняя версия модели Qwen-VL, которая достигла передовых результатов в тестировании визуального понимания."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 — это новая серия крупных языковых моделей, предназначенная для оптимизации обработки инструктивных задач."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 — это новая серия крупных языковых моделей, предназначенная для оптимизации обработки инструктивных задач."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Большая языковая модель, разработанная командой Alibaba Cloud Tongyi Qianwen."
  },
  "Qwen/Qwen2.5-72B-Instruct-128K": {
    "description": "Qwen2.5 - это новая серия крупных языковых моделей с улучшенными способностями понимания и генерации."
  },
  "Qwen/Qwen2.5-72B-Instruct-Turbo": {
    "description": "Qwen2.5 - это новая серия крупных языковых моделей, нацеленная на оптимизацию обработки задач с инструкциями."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 — это новая серия крупных языковых моделей, предназначенная для оптимизации обработки инструктивных задач."
  },
  "Qwen/Qwen2.5-7B-Instruct-Turbo": {
    "description": "Qwen2.5 - это новая серия крупных языковых моделей, нацеленная на оптимизацию обработки задач с инструкциями."
  },
  "Qwen/Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder сосредоточен на написании кода."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder-7B-Instruct — это последняя версия серии языковых моделей, специфичных для кода, выпущенная Alibaba Cloud. Эта модель значительно улучшила способности генерации кода, вывода и исправления на основе Qwen2.5, обучаясь на 5.5 триллионах токенов. Она не только усилила кодирование, но и сохранила преимущества в математике и общих способностях. Модель предоставляет более полную основу для практических приложений, таких как интеллектуальные агенты кода."
  },
  "Qwen2-72B-Instruct": {
    "description": "Qwen2 — это последняя серия моделей Qwen, поддерживающая контекст до 128k. По сравнению с текущими лучшими открытыми моделями, Qwen2-72B значительно превосходит ведущие модели по многим аспектам, включая понимание естественного языка, знания, код, математику и многоязычность."
  },
  "Qwen2-7B-Instruct": {
    "description": "Qwen2 — это последняя серия моделей Qwen, способная превосходить лучшие открытые модели сопоставимого размера и даже более крупные модели. Qwen2 7B демонстрирует значительные преимущества в нескольких тестах, особенно в понимании кода и китайского языка."
  },
  "Qwen2-VL-72B": {
    "description": "Qwen2-VL-72B — это мощная модель визуального языка, поддерживающая многомодальную обработку изображений и текста, способная точно распознавать содержимое изображений и генерировать соответствующие описания или ответы."
  },
  "Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5-14B-Instruct — это языковая модель с 14 миллиардами параметров, с отличными показателями производительности, оптимизированная для китайского и многоязычного контекста, поддерживает интеллектуальные ответы, генерацию контента и другие приложения."
  },
  "Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5-32B-Instruct — это языковая модель с 32 миллиардами параметров, с сбалансированными показателями производительности, оптимизированная для китайского и многоязычного контекста, поддерживает интеллектуальные ответы, генерацию контента и другие приложения."
  },
  "Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct поддерживает контекст до 16k, генерируя длинные тексты более 8K. Поддерживает вызовы функций и бесшовное взаимодействие с внешними системами, что значительно повышает гибкость и масштабируемость. Знания модели значительно увеличены, а способности в кодировании и математике значительно улучшены, поддерживает более 29 языков."
  },
  "Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5-7B-Instruct — это языковая модель с 7 миллиардами параметров, поддерживающая вызовы функций и бесшовное взаимодействие с внешними системами, что значительно повышает гибкость и масштабируемость. Оптимизирована для китайского и многоязычного контекста, поддерживает интеллектуальные ответы, генерацию контента и другие приложения."
  },
  "Qwen2.5-Coder-14B-Instruct": {
    "description": "Qwen2.5-Coder-14B-Instruct — это модель программирования на основе масштабного предварительного обучения, обладающая мощными способностями к пониманию и генерации кода, способная эффективно решать различные задачи программирования, особенно подходит для интеллектуального написания кода, автоматизации скриптов и ответов на программные вопросы."
  },
  "Qwen2.5-Coder-32B-Instruct": {
    "description": "Qwen2.5-Coder-32B-Instruct — это крупная языковая модель, специально разработанная для генерации кода, понимания кода и эффективных сценариев разработки, с передовым масштабом параметров 32B, способная удовлетворить разнообразные потребности программирования."
  },
  "SenseChat": {
    "description": "Базовая версия модели (V4), длина контекста 4K, обладает мощными универсальными возможностями."
  },
  "SenseChat-128K": {
    "description": "Базовая версия модели (V4), длина контекста 128K, демонстрирует отличные результаты в задачах понимания и генерации длинных текстов."
  },
  "SenseChat-32K": {
    "description": "Базовая версия модели (V4), длина контекста 32K, гибко применяется в различных сценариях."
  },
  "SenseChat-5": {
    "description": "Последняя версия модели (V5.5), длина контекста 128K, значительно улучшенные способности в математическом рассуждении, английских диалогах, следовании инструкциям и понимании длинных текстов, сопоставимые с GPT-4o."
  },
  "SenseChat-5-1202": {
    "description": "Это последняя версия на основе V5.5, которая значительно улучшила свои базовые способности в китайском и английском языках, общении, научных знаниях, гуманитарных знаниях, написании, математической логике и контроле количества слов по сравнению с предыдущей версией."
  },
  "SenseChat-5-Cantonese": {
    "description": "Длина контекста 32K, превосходит GPT-4 в понимании диалогов на кантонском, сопоставим с GPT-4 Turbo в таких областях, как знания, рассуждение, математика и написание кода."
  },
  "SenseChat-Character": {
    "description": "Стандартная версия модели, длина контекста 8K, высокая скорость отклика."
  },
  "SenseChat-Character-Pro": {
    "description": "Расширенная версия модели, длина контекста 32K, всеобъемлющие улучшения возможностей, поддерживает диалоги на китайском и английском языках."
  },
  "SenseChat-Turbo": {
    "description": "Подходит для быстрого ответа на вопросы и сценариев тонкой настройки модели."
  },
  "SenseChat-Turbo-1202": {
    "description": "Это последняя легковесная версия модели, которая достигает более 90% возможностей полной модели и значительно снижает затраты на вывод."
  },
  "SenseChat-Vision": {
    "description": "Последняя версия модели (V5.5) поддерживает ввод нескольких изображений, полностью реализует оптимизацию базовых возможностей модели и значительно улучшила распознавание свойств объектов, пространственные отношения, распознавание событий, понимание сцен, распознавание эмоций, логическое рассуждение и понимание текста."
  },
  "Skylark2-lite-8k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-lite имеет высокую скорость отклика, подходит для сценариев с высокими требованиями к оперативности, чувствительных к стоимости и с не такими высокими требованиями к точности модели. Длина контекстного окна составляет 8k."
  },
  "Skylark2-pro-32k": {
    "description": "Модель второго поколения Skylark (云雀), версия Skylark2-pro имеет высокую точность модели, подходит для более сложных сценариев генерации текста, таких как написание специализированной документации, создание романов, высококачественный перевод и т.д. Длина контекстного окна составляет 32k."
  },
  "Skylark2-pro-4k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-pro имеет высокую точность, подходит для более сложных сценариев генерации текста, таких как специализированная документация, создание романов, высококачественный перевод и т.д. Длина контекстного окна составляет 4k."
  },
  "Skylark2-pro-character-4k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-pro-character демонстрирует выдающиеся способности к ролевым взаимодействиям и чатам, умеет играть различные роли в зависимости от требований пользователя, что делает общение естественным и плавным. Подходит для разработки чат-ботов, виртуальных помощников и онлайн-сервисов с высокой скоростью отклика."
  },
  "Skylark2-pro-turbo-8k": {
    "description": "Модель второго поколения Skylark (云雀), модель Skylark2-pro-turbo-8k обеспечивает более быструю обработку и сниженные затраты, длина контекстного окна составляет 8k."
  },
  "THUDM/chatglm3-6b": {
    "description": "ChatGLM3-6B — это открытая модель из серии ChatGLM, разработанная Zhizhu AI. Эта модель сохраняет отличные характеристики предыдущих моделей, такие как плавность диалога и низкий порог развертывания, одновременно вводя новые функции. Она использует более разнообразные обучающие данные, большее количество шагов обучения и более разумную стратегию обучения, показывая отличные результаты среди предобученных моделей объемом менее 10B. ChatGLM3-6B поддерживает многократные диалоги, вызовы инструментов, выполнение кода и задачи агента в сложных сценариях. Кроме диалоговой модели, также открыты базовая модель ChatGLM-6B-Base и модель для длинных текстовых диалогов ChatGLM3-6B-32K. Эта модель полностью открыта для академических исследований и также допускает бесплатное коммерческое использование после регистрации."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B — это открытая версия, обеспечивающая оптимизированный диалоговый опыт для приложений."
  },
  "TeleAI/TeleChat2": {
    "description": "Модель TeleChat2 была разработана China Telecom с нуля и представляет собой генеративную семантическую модель, поддерживающую функции вопросов и ответов, генерации кода, генерации длинных текстов и т.д., предоставляя пользователям услуги консультаций в диалоговом формате, способную взаимодействовать с пользователями, отвечать на вопросы, помогать в творчестве и эффективно помогать пользователям получать информацию, знания и вдохновение. Модель показывает отличные результаты в решении проблем с галлюцинациями, генерацией длинных текстов и логическим пониманием."
  },
  "TeleAI/TeleMM": {
    "description": "Модель TeleMM — это многомодальная модель, разработанная China Telecom, способная обрабатывать текстовые, графические и другие виды входных данных, поддерживающая функции понимания изображений, анализа графиков и т.д., предоставляя пользователям услуги понимания на разных модальностях. Модель может взаимодействовать с пользователями в многомодальном формате, точно понимая входной контент, отвечая на вопросы, помогая в творчестве и эффективно предоставляя многомодальную информацию и поддержку вдохновения. Она показывает отличные результаты в задачах многомодального восприятия и логического вывода."
  },
  "Vendor-A/Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5-72B-Instruct — это одна из последних языковых моделей, выпущенных Alibaba Cloud. Эта 72B модель значительно улучшила способности в области кодирования и математики. Модель также поддерживает множество языков, охватывающих более 29 языков, включая китайский и английский. Она значительно улучшила выполнение инструкций, понимание структурированных данных и генерацию структурированных выходных данных (особенно JSON)."
  },
  "Yi-34B-Chat": {
    "description": "Yi-1.5-34B, сохраняя выдающиеся универсальные языковые способности оригинальной серии моделей, значительно улучшила математическую логику и способности к кодированию благодаря инкрементальному обучению на 500 миллиардов высококачественных токенов."
  },
  "abab5.5-chat": {
    "description": "Ориентирован на производственные сценарии, поддерживает обработку сложных задач и эффективную генерацию текста, подходит для профессиональных приложений."
  },
  "abab5.5s-chat": {
    "description": "Специально разработан для диалогов на китайском языке, обеспечивая высококачественную генерацию диалогов на китайском, подходит для различных приложений."
  },
  "abab6.5g-chat": {
    "description": "Специально разработан для многоязычных диалогов, поддерживает высококачественную генерацию диалогов на английском и других языках."
  },
  "abab6.5s-chat": {
    "description": "Подходит для широкого спектра задач обработки естественного языка, включая генерацию текста, диалоговые системы и т.д."
  },
  "abab6.5t-chat": {
    "description": "Оптимизирован для диалогов на китайском языке, обеспечивая плавную генерацию диалогов, соответствующую китайским языковым привычкам."
  },
  "accounts/fireworks/models/deepseek-r1": {
    "description": "DeepSeek-R1 — это передовая большая языковая модель, оптимизированная с помощью обучения с подкреплением и холодных стартовых данных, обладающая выдающимися показателями вывода, математики и программирования."
  },
  "accounts/fireworks/models/deepseek-v3": {
    "description": "Мощная языковая модель Mixture-of-Experts (MoE) от Deepseek с общим количеством параметров 671B, активирующая 37B параметров на каждый токен."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "Модель Llama 3 70B для команд, специально оптимизированная для многоязычных диалогов и понимания естественного языка, превосходит большинство конкурентных моделей."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "Модель Llama 3 8B для команд, оптимизированная для диалогов и многоязычных задач, демонстрирует выдающиеся и эффективные результаты."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "Модель Llama 3 8B для команд (HF версия), результаты которой совпадают с официальной реализацией, обладает высокой согласованностью и совместимостью между платформами."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "Модель Llama 3.1 405B для команд, обладающая огромным количеством параметров, подходит для сложных задач и сценариев с высокой нагрузкой."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "Модель Llama 3.1 70B для команд, обеспечивающая выдающиеся возможности понимания и генерации естественного языка, является идеальным выбором для диалоговых и аналитических задач."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "Модель Llama 3.1 8B для команд, оптимизированная для многоязычных диалогов, способная превосходить большинство открытых и закрытых моделей по общим отраслевым стандартам."
  },
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct": {
    "description": "Модель Meta с 11B параметрами, оптимизированная для вывода изображений. Эта модель предназначена для визуального распознавания, вывода изображений, описания изображений и ответа на общие вопросы о изображениях. Эта модель способна понимать визуальные данные, такие как графики и диаграммы, и преодолевать разрыв между визуальным и языковым пониманием, генерируя текстовые описания деталей изображений."
  },
  "accounts/fireworks/models/llama-v3p2-3b-instruct": {
    "description": "Модель Llama 3.2 3B для инструкций - это компактная многоязычная модель, запущенная Meta. Эта модель предназначена для повышения эффективности и обеспечивает значительное улучшение в задержке и стоимости по сравнению с более крупными моделями. Примеры использования модели включают запросы, переоформление подсказок и помощь в написании."
  },
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct": {
    "description": "Модель Meta с 90B параметрами, оптимизированная для вывода изображений. Эта модель предназначена для визуального распознавания, вывода изображений, описания изображений и ответа на общие вопросы о изображениях. Эта модель способна понимать визуальные данные, такие как графики и диаграммы, и преодолевать разрыв между визуальным и языковым пониманием, генерируя текстовые описания деталей изображений."
  },
  "accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "description": "Llama 3.3 70B Instruct — это обновленная версия Llama 3.1 70B от декабря. Эта модель улучшена на основе Llama 3.1 70B (выпущенной в июле 2024 года), с усиленной поддержкой вызовов инструментов, многоязычного текста, математических и программных возможностей. Модель достигла ведущих в отрасли показателей в области вывода, математики и соблюдения инструкций, обеспечивая производительность, сопоставимую с 3.1 405B, при этом обладая значительными преимуществами по скорости и стоимости."
  },
  "accounts/fireworks/models/mistral-small-24b-instruct-2501": {
    "description": "Модель с 24B параметрами, обладающая передовыми возможностями, сопоставимыми с более крупными моделями."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "Mixtral MoE 8x22B для команд, с большим количеством параметров и архитектурой с несколькими экспертами, всесторонне поддерживает эффективную обработку сложных задач."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "Mixtral MoE 8x7B для команд, архитектура с несколькими экспертами обеспечивает эффективное выполнение и следование командам."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "Модель MythoMax L2 13B, использующая новые технологии объединения, хорошо подходит для повествования и ролевых игр."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "Phi 3 Vision для команд, легковесная мультимодальная модель, способная обрабатывать сложную визуальную и текстовую информацию, обладая высокой способностью к выводу."
  },
  "accounts/fireworks/models/qwen-qwq-32b-preview": {
    "description": "Модель QwQ — это экспериментальная исследовательская модель, разработанная командой Qwen, сосредоточенная на улучшении возможностей вывода ИИ."
  },
  "accounts/fireworks/models/qwen2-vl-72b-instruct": {
    "description": "72B версия модели Qwen-VL — это результат последней итерации Alibaba, представляющий собой инновации почти за год."
  },
  "accounts/fireworks/models/qwen2p5-72b-instruct": {
    "description": "Qwen2.5 - это серия языковых моделей, содержащая только декодеры, разработанная командой Qwen от Alibaba Cloud. Эти модели предлагаются в различных размерах: 0.5B, 1.5B, 3B, 7B, 14B, 32B и 72B, с вариантами базовой и инструкционной версии."
  },
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "description": "Qwen2.5 Coder 32B Instruct — это последняя версия серии языковых моделей, специфичных для кода, выпущенная Alibaba Cloud. Эта модель значительно улучшила способности генерации кода, вывода и исправления на основе Qwen2.5, обучаясь на 5.5 триллионах токенов. Она не только усилила кодирование, но и сохранила преимущества в математике и общих способностях. Модель предоставляет более полную основу для практических приложений, таких как интеллектуальные агенты кода."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "Модель Yi-Large, обладающая выдающимися возможностями обработки нескольких языков, подходит для различных задач генерации и понимания языка."
  },
  "ai21-jamba-1.5-large": {
    "description": "Многоязычная модель с 398B параметрами (94B активных), предлагающая контекстное окно длиной 256K, вызовы функций, структурированный вывод и основанное на фактах генерирование."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Многоязычная модель с 52B параметрами (12B активных), предлагающая контекстное окно длиной 256K, вызовы функций, структурированный вывод и основанное на фактах генерирование."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "Claude 3.5 Sonnet устанавливает новые отраслевые стандарты, превосходя модели конкурентов и Claude 3 Opus, демонстрируя отличные результаты в широком спектре оценок, при этом обладая скоростью и стоимостью наших моделей среднего уровня."
  },
  "anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet установил новые стандарты в отрасли, превзойдя модели конкурентов и Claude 3 Opus, продемонстрировав отличные результаты в широкомасштабных оценках, при этом обладая скоростью и стоимостью наших моделей среднего уровня."
  },
  "anthropic.claude-3-7-sonnet-20250219-v1:0": {
    "description": "Claude 3.7 Sonnet — это самая мощная модель от Anthropic, обладающая передовыми характеристиками в области высоко сложных задач. Она может обрабатывать открытые подсказки и невидимые сценарии, демонстрируя отличную плавность и человеческое понимание. Claude 3.7 Sonnet демонстрирует передовые возможности генеративного AI. Claude 3.7 Sonnet может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "Claude 3 Haiku — это самая быстрая и компактная модель от Anthropic, обеспечивающая почти мгновенную скорость ответа. Она может быстро отвечать на простые запросы и запросы. Клиенты смогут создать бесшовный AI-опыт, имитирующий человеческое взаимодействие. Claude 3 Haiku может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "Claude 3 Opus — это самый мощный AI-модель от Anthropic, обладающая передовыми характеристиками в области высоко сложных задач. Она может обрабатывать открытые подсказки и невидимые сценарии, демонстрируя отличную плавность и человеческое понимание. Claude 3 Opus демонстрирует передовые возможности генеративного AI. Claude 3 Opus может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "Claude 3 Sonnet от Anthropic достигает идеального баланса между интеллектом и скоростью — особенно подходит для корпоративных рабочих нагрузок. Он предлагает максимальную полезность по цене ниже конкурентов и разработан как надежный, высокопрочный основной механизм для масштабируемых AI-развертываний. Claude 3 Sonnet может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Быстрая, экономичная и все еще очень мощная модель, способная обрабатывать широкий спектр задач, включая повседневные диалоги, текстовый анализ, резюме и вопросы к документам."
  },
  "anthropic.claude-v2": {
    "description": "Модель Anthropic демонстрирует высокие способности в широком спектре задач, от сложных диалогов и генерации креативного контента до детального следования инструкциям."
  },
  "anthropic.claude-v2:1": {
    "description": "Обновленная версия Claude 2, обладающая двойным контекстным окном и улучшениями в надежности, уровне галлюцинаций и точности на основе доказательств в длинных документах и контексте RAG."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku — это самая быстрая и компактная модель от Anthropic, предназначенная для почти мгновенных ответов. Она обладает быстрой и точной направленной производительностью."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus — это самая мощная модель от Anthropic для обработки высококомплексных задач. Она демонстрирует выдающиеся результаты по производительности, интеллекту, плавности и пониманию."
  },
  "anthropic/claude-3.5-haiku": {
    "description": "Claude 3.5 Haiku — это самая быстрая модель следующего поколения от Anthropic. По сравнению с Claude 3 Haiku, Claude 3.5 Haiku продемонстрировала улучшения во всех навыках и превзошла предыдущую крупнейшую модель Claude 3 Opus во многих интеллектуальных бенчмарках."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet предлагает возможности, превосходящие Opus, и скорость, превышающую Sonnet, при этом сохраняя ту же цену. Sonnet особенно хорошо справляется с программированием, наукой о данных, визуальной обработкой и агентскими задачами."
  },
  "aya": {
    "description": "Aya 23 — это многоязычная модель, выпущенная Cohere, поддерживающая 23 языка, обеспечивая удобство для многоязычных приложений."
  },
  "aya:35b": {
    "description": "Aya 23 — это многоязычная модель, выпущенная Cohere, поддерживающая 23 языка, обеспечивая удобство для многоязычных приложений."
  },
  "charglm-3": {
    "description": "CharGLM-3 разработан для ролевых игр и эмоционального сопровождения, поддерживает сверхдлинную многократную память и персонализированные диалоги, имеет широкое применение."
  },
  "chatgpt-4o-latest": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "claude-2.0": {
    "description": "Claude 2 предлагает ключевые улучшения для бизнеса, включая ведущие в отрасли 200K токенов контекста, значительное снижение частоты галлюцинаций модели, системные подсказки и новую тестовую функцию: вызов инструментов."
  },
  "claude-2.1": {
    "description": "Claude 2 предлагает ключевые улучшения для бизнеса, включая ведущие в отрасли 200K токенов контекста, значительное снижение частоты галлюцинаций модели, системные подсказки и новую тестовую функцию: вызов инструментов."
  },
  "claude-3-5-haiku-20241022": {
    "description": "Claude 3.5 Haiku — это самая быстрая следующая модель от Anthropic. По сравнению с Claude 3 Haiku, Claude 3.5 Haiku продемонстрировала улучшения во всех навыках и превзошла предыдущую крупнейшую модель Claude 3 Opus во многих интеллектуальных тестах."
  },
  "claude-3-5-sonnet-20240620": {
    "description": "Claude 3.5 Sonnet предлагает возможности, превосходящие Opus, и скорость, быстрее Sonnet, при этом сохраняя ту же цену. Sonnet особенно хорош в программировании, науке о данных, визуальной обработке и задачах агентов."
  },
  "claude-3-5-sonnet-20241022": {
    "description": "Claude 3.5 Sonnet предлагает возможности, превышающие Opus, и скорость, превышающую Sonnet, при этом сохраняя ту же цену. Sonnet особенно хорош в программировании, данных, визуальной обработке и代理задачах."
  },
  "claude-3-7-sonnet-20250219": {
    "description": "Claude 3.7 Sonnet — это самая мощная модель от Anthropic, обладающая передовыми характеристиками в области высоко сложных задач. Она может обрабатывать открытые подсказки и невидимые сценарии, демонстрируя отличную плавность и человеческое понимание. Claude 3.7 Sonnet демонстрирует передовые возможности генеративного AI. Claude 3.7 Sonnet может обрабатывать изображения и возвращать текстовый вывод, имея контекстное окно в 200K."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku — это самая быстрая и компактная модель от Anthropic, предназначенная для достижения почти мгновенных ответов. Она обладает быстрой и точной направленной производительностью."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus — это самая мощная модель от Anthropic для обработки высококомплексных задач. Она демонстрирует выдающиеся результаты по производительности, интеллекту, плавности и пониманию."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet обеспечивает идеальный баланс между интеллектом и скоростью для корпоративных рабочих нагрузок. Он предлагает максимальную полезность по более низкой цене, надежен и подходит для масштабного развертывания."
  },
  "codegeex-4": {
    "description": "CodeGeeX-4 — это мощный AI помощник по программированию, поддерживающий интеллектуальные ответы и автозаполнение кода на различных языках программирования, повышая эффективность разработки."
  },
  "codegeex4-all-9b": {
    "description": "CodeGeeX4-ALL-9B — это многоязычная модель генерации кода, поддерживающая полный спектр функций, включая автозаполнение и генерацию кода, интерпретатор кода, веб-поиск, вызовы функций и вопросы по коду на уровне репозитория, охватывающая различные сценарии разработки программного обеспечения. Это одна из лучших моделей генерации кода с количеством параметров менее 10B."
  },
  "codegemma": {
    "description": "CodeGemma — это легковесная языковая модель, специально разработанная для различных задач программирования, поддерживающая быструю итерацию и интеграцию."
  },
  "codegemma:2b": {
    "description": "CodeGemma — это легковесная языковая модель, специально разработанная для различных задач программирования, поддерживающая быструю итерацию и интеграцию."
  },
  "codellama": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codellama/CodeLlama-34b-Instruct-hf": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, с поддержкой широкого спектра языков программирования, подходящая для среды разработчиков."
  },
  "codellama:13b": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codellama:34b": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codellama:70b": {
    "description": "Code Llama — это LLM, сосредоточенная на генерации и обсуждении кода, поддерживающая широкий спектр языков программирования, подходит для среды разработчиков."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 — это крупномасштабная языковая модель, обученная на большом объёме кодовых данных, специально разработанная для решения сложных задач программирования."
  },
  "codestral": {
    "description": "Codestral — это первая модель кода от Mistral AI, обеспечивающая отличную поддержку для задач генерации кода."
  },
  "codestral-latest": {
    "description": "Codestral — это передовая генеративная модель, сосредоточенная на генерации кода, оптимизированная для промежуточного заполнения и задач дополнения кода."
  },
  "cognitivecomputations/dolphin-mixtral-8x22b": {
    "description": "Dolphin Mixtral 8x22B — это модель, разработанная для соблюдения инструкций, диалогов и программирования."
  },
  "cohere-command-r": {
    "description": "Command R — это масштабируемая генеративная модель, нацеленная на RAG и использование инструментов для обеспечения AI на уровне производства для предприятий."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ — это модель, оптимизированная для RAG, предназначенная для решения задач корпоративного уровня."
  },
  "command-r": {
    "description": "Command R — это LLM, оптимизированная для диалогов и задач с длинным контекстом, особенно подходит для динамического взаимодействия и управления знаниями."
  },
  "command-r-plus": {
    "description": "Command R+ — это высокопроизводительная большая языковая модель, специально разработанная для реальных бизнес-сценариев и сложных приложений."
  },
  "dall-e-2": {
    "description": "Вторая генерация модели DALL·E, поддерживающая более реалистичную и точную генерацию изображений с разрешением в 4 раза выше, чем у первой генерации."
  },
  "dall-e-3": {
    "description": "Последняя модель DALL·E, выпущенная в ноябре 2023 года. Поддерживает более реалистичную и точную генерацию изображений с более сильной детализацией."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct предлагает высокую надежность в обработке команд, поддерживая приложения в различных отраслях."
  },
  "deepseek-ai/DeepSeek-R1": {
    "description": "DeepSeek-R1 — это модель вывода, управляемая методом обучения с подкреплением (RL), которая решает проблемы повторяемости и читаемости модели. Перед применением RL DeepSeek-R1 вводит данные холодного старта, что дополнительно оптимизирует производительность вывода. Она показывает сопоставимые результаты с OpenAI-o1 в математических, кодовых и задачах вывода, а также улучшает общую эффективность благодаря тщательно разработанным методам обучения."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
    "description": "Модель DeepSeek-R1, дистиллированная с помощью усиленного обучения и данных холодного старта, оптимизирует производительность вывода, обновляя стандарт многозадачности в открытых моделях."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B": {
    "description": "DeepSeek-R1-Distill-Llama-8B — это дистиллированная модель, основанная на Llama-3.1-8B. Эта модель была дообучена на образцах, сгенерированных DeepSeek-R1, и демонстрирует отличные способности вывода. Она показала хорошие результаты в нескольких бенчмарках, включая 89.1% точности на MATH-500, 50.4% проходной уровень на AIME 2024 и 1205 баллов на CodeForces, демонстрируя сильные математические и программные способности для модели объемом 8B."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
    "description": "Модель DeepSeek-R1, дистиллированная с помощью усиленного обучения и данных холодного старта, оптимизирует производительность вывода, обновляя стандарт многозадачности в открытых моделях."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
    "description": "Модель DeepSeek-R1, дистиллированная с помощью усиленного обучения и данных холодного старта, оптимизирует производительность вывода, обновляя стандарт многозадачности в открытых моделях."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
    "description": "DeepSeek-R1-Distill-Qwen-32B — это модель, полученная с помощью дистилляции на основе Qwen2.5-32B. Эта модель была дообучена на 800000 отобранных образцах, сгенерированных DeepSeek-R1, и демонстрирует выдающуюся производительность в таких областях, как математика, программирование и логика. Она показала отличные результаты в нескольких бенчмарках, включая AIME 2024, MATH-500 и GPQA Diamond, достигнув 94.3% точности на MATH-500, демонстрируя мощные способности математического вывода."
  },
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
    "description": "DeepSeek-R1-Distill-Qwen-7B — это модель, полученная с помощью дистилляции на основе Qwen2.5-Math-7B. Эта модель была дообучена на 800000 отобранных образцах, сгенерированных DeepSeek-R1, и демонстрирует отличные способности вывода. Она показала выдающиеся результаты в нескольких бенчмарках, включая 92.8% точности на MATH-500, 55.5% проходной уровень на AIME 2024 и 1189 баллов на CodeForces, демонстрируя сильные математические и программные способности для модели объемом 7B."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 объединяет отличительные черты предыдущих версий, улучшая общие и кодировочные способности."
  },
  "deepseek-ai/DeepSeek-V3": {
    "description": "DeepSeek-V3 — это языковая модель смешанных экспертов (MoE) с 6710 миллиардами параметров, использующая многоголовое потенциальное внимание (MLA) и архитектуру DeepSeekMoE, в сочетании с стратегией балансировки нагрузки без вспомогательных потерь, оптимизирующей эффективность вывода и обучения. После предобучения на 14,8 триллионах высококачественных токенов и последующей супервизионной донастройки и обучения с подкреплением, DeepSeek-V3 превосходит другие открытые модели и приближается к ведущим закрытым моделям."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B — это передовая модель, обученная для высококомплексных диалогов."
  },
  "deepseek-ai/deepseek-r1": {
    "description": "Современная эффективная LLM, специализирующаяся на рассуждениях, математике и программировании."
  },
  "deepseek-ai/deepseek-vl2": {
    "description": "DeepSeek-VL2 — это модель визуального языка, разработанная на основе DeepSeekMoE-27B, использующая архитектуру MoE с разреженной активацией, которая демонстрирует выдающуюся производительность при активации всего 4,5 миллиарда параметров. Эта модель показывает отличные результаты в таких задачах, как визуальные вопросы и ответы, оптическое распознавание символов, понимание документов/таблиц/графиков и визуальная локализация."
  },
  "deepseek-chat": {
    "description": "Новая открытая модель, объединяющая общие и кодовые возможности, не только сохраняет общие диалоговые способности оригинальной модели Chat и мощные возможности обработки кода модели Coder, но и лучше согласуется с человеческими предпочтениями. Кроме того, DeepSeek-V2.5 значительно улучшила производительность в таких задачах, как написание текстов и следование инструкциям."
  },
  "deepseek-coder-33B-instruct": {
    "description": "DeepSeek Coder 33B — это модель языкового кодирования, обученная на 20 триллионах данных, из которых 87% составляют код, а 13% — китайский и английский языки. Модель использует размер окна 16K и задачи заполнения пропусков, предоставляя функции автозаполнения кода и заполнения фрагментов на уровне проектов."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 — это открытая смешанная экспертная модель кода, показывающая отличные результаты в задачах кода, сопоставимая с GPT4-Turbo."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 — это открытая смешанная экспертная модель кода, показывающая отличные результаты в задачах кода, сопоставимая с GPT4-Turbo."
  },
  "deepseek-r1": {
    "description": "DeepSeek-R1 — это модель вывода, управляемая методом обучения с подкреплением (RL), которая решает проблемы повторяемости и читаемости модели. Перед применением RL DeepSeek-R1 вводит данные холодного старта, что дополнительно оптимизирует производительность вывода. Она показывает сопоставимые результаты с OpenAI-o1 в математических, кодовых и задачах вывода, а также улучшает общую эффективность благодаря тщательно разработанным методам обучения."
  },
  "deepseek-r1-distill-llama-70b": {
    "description": "DeepSeek R1 — более крупная и умная модель в наборе DeepSeek, была дистиллирована в архитектуру Llama 70B. На основе бенчмарков и ручной оценки эта модель более умная, особенно в задачах, требующих математической и фактической точности."
  },
  "deepseek-r1-distill-llama-8b": {
    "description": "Модели серии DeepSeek-R1-Distill были получены с помощью технологии дистилляции знаний, донастраивая образцы, сгенерированные DeepSeek-R1, на открытых моделях, таких как Qwen и Llama."
  },
  "deepseek-r1-distill-qwen-1.5b": {
    "description": "Модели серии DeepSeek-R1-Distill были получены с помощью технологии дистилляции знаний, донастраивая образцы, сгенерированные DeepSeek-R1, на открытых моделях, таких как Qwen и Llama."
  },
  "deepseek-r1-distill-qwen-14b": {
    "description": "Модели серии DeepSeek-R1-Distill были получены с помощью технологии дистилляции знаний, донастраивая образцы, сгенерированные DeepSeek-R1, на открытых моделях, таких как Qwen и Llama."
  },
  "deepseek-r1-distill-qwen-32b": {
    "description": "Модели серии DeepSeek-R1-Distill были получены с помощью технологии дистилляции знаний, донастраивая образцы, сгенерированные DeepSeek-R1, на открытых моделях, таких как Qwen и Llama."
  },
  "deepseek-r1-distill-qwen-7b": {
    "description": "Модели серии DeepSeek-R1-Distill были получены с помощью технологии дистилляции знаний, донастраивая образцы, сгенерированные DeepSeek-R1, на открытых моделях, таких как Qwen и Llama."
  },
  "deepseek-reasoner": {
    "description": "Модель вывода, представленная DeepSeek. Перед тем как выдать окончательный ответ, модель сначала выводит цепочку размышлений, чтобы повысить точность окончательного ответа."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 — это эффективная языковая модель Mixture-of-Experts, подходящая для экономически эффективных потребностей обработки."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B — это модель кода DeepSeek, обеспечивающая мощные возможности генерации кода."
  },
  "deepseek-v3": {
    "description": "DeepSeek-V3 — это модель MoE, разработанная компанией Hangzhou DeepSeek AI Technology Research Co., Ltd., которая показывает выдающиеся результаты в нескольких тестах и занимает первое место среди открытых моделей в основных рейтингах. V3 по сравнению с моделью V2.5 увеличила скорость генерации в 3 раза, обеспечивая пользователям более быстрое и плавное использование."
  },
  "deepseek/deepseek-chat": {
    "description": "Новая открытая модель, объединяющая общие и кодовые возможности, не только сохраняет общие диалоговые способности оригинальной модели Chat и мощные возможности обработки кода модели Coder, но и лучше соответствует человеческим предпочтениям. Кроме того, DeepSeek-V2.5 значительно улучшила свои результаты в задачах написания, следования инструкциям и других областях."
  },
  "deepseek/deepseek-r1": {
    "description": "DeepSeek-R1 значительно улучшила способности модели к рассуждению при наличии лишь очень ограниченных размеченных данных. Перед тем как предоставить окончательный ответ, модель сначала выводит цепочку размышлений, чтобы повысить точность окончательного ответа."
  },
  "deepseek/deepseek-r1:free": {
    "description": "DeepSeek-R1 значительно улучшила способности модели к рассуждению при наличии лишь очень ограниченных размеченных данных. Перед тем как предоставить окончательный ответ, модель сначала выводит цепочку размышлений, чтобы повысить точность окончательного ответа."
  },
  "doubao-1.5-lite-32k": {
    "description": "Doubao-1.5-lite - совершенно новое поколение легкой модели, с максимальной скоростью отклика, результаты и задержка достигают мирового уровня."
  },
  "doubao-1.5-pro-256k": {
    "description": "Doubao-1.5-pro-256k основан на полностью обновленной версии Doubao-1.5-Pro, с общим улучшением на 10%. Поддерживает вывод на 256k контекстных окнах, максимальная длина вывода составляет 12k токенов. Более высокая производительность, большее окно, отличное соотношение цена-качество, подходит для более широкого спектра приложений."
  },
  "doubao-1.5-pro-32k": {
    "description": "Doubao-1.5-pro - совершенно новое поколение основного модели, с полностью обновленной производительностью, выдающимися результатами в области знаний, кода, логики и других аспектов."
  },
  "emohaa": {
    "description": "Emohaa — это психологическая модель, обладающая профессиональными консультационными способностями, помогающая пользователям понимать эмоциональные проблемы."
  },
  "ernie-3.5-128k": {
    "description": "Флагманская большая языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными способностями, способная удовлетворить требования большинства сценариев диалогов, генерации контента и применения плагинов; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации."
  },
  "ernie-3.5-8k": {
    "description": "Флагманская большая языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными способностями, способная удовлетворить требования большинства сценариев диалогов, генерации контента и применения плагинов; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации."
  },
  "ernie-3.5-8k-preview": {
    "description": "Флагманская большая языковая модель, разработанная Baidu, охватывающая огромные объемы китайских и английских текстов, обладающая мощными универсальными способностями, способная удовлетворить требования большинства сценариев диалогов, генерации контента и применения плагинов; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации."
  },
  "ernie-4.0-8k-latest": {
    "description": "Флагманская сверхбольшая языковая модель, разработанная Baidu, по сравнению с ERNIE 3.5 демонстрирует полное обновление возможностей модели, широко применима в сложных задачах различных областей; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации."
  },
  "ernie-4.0-8k-preview": {
    "description": "Флагманская сверхбольшая языковая модель, разработанная Baidu, по сравнению с ERNIE 3.5 демонстрирует полное обновление возможностей модели, широко применима в сложных задачах различных областей; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации."
  },
  "ernie-4.0-turbo-128k": {
    "description": "Флагманская сверхбольшая языковая модель, разработанная Baidu, демонстрирует отличные результаты в комплексных задачах, широко применима в различных областях; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации. По сравнению с ERNIE 4.0, она показывает лучшие результаты."
  },
  "ernie-4.0-turbo-8k-latest": {
    "description": "Флагманская сверхбольшая языковая модель, разработанная Baidu, демонстрирует отличные результаты в комплексных задачах, широко применима в различных областях; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации. По сравнению с ERNIE 4.0, она показывает лучшие результаты."
  },
  "ernie-4.0-turbo-8k-preview": {
    "description": "Флагманская сверхбольшая языковая модель, разработанная Baidu, демонстрирует отличные результаты в комплексных задачах, широко применима в различных областях; поддерживает автоматическое подключение к плагину поиска Baidu, обеспечивая актуальность информации. По сравнению с ERNIE 4.0, она показывает лучшие результаты."
  },
  "ernie-char-8k": {
    "description": "Специализированная большая языковая модель, разработанная Baidu, подходящая для применения в игровых NPC, диалогах службы поддержки, ролевых играх и других сценариях, с более ярким и последовательным стилем персонажей, более высокой способностью следовать инструкциям и лучшей производительностью вывода."
  },
  "ernie-char-fiction-8k": {
    "description": "Специализированная большая языковая модель, разработанная Baidu, подходящая для применения в игровых NPC, диалогах службы поддержки, ролевых играх и других сценариях, с более ярким и последовательным стилем персонажей, более высокой способностью следовать инструкциям и лучшей производительностью вывода."
  },
  "ernie-lite-8k": {
    "description": "ERNIE Lite — это легковесная большая языковая модель, разработанная Baidu, которая сочетает в себе отличные результаты модели и производительность вывода, подходит для использования на AI-ускорителях с низкой вычислительной мощностью."
  },
  "ernie-lite-pro-128k": {
    "description": "Легковесная большая языковая модель, разработанная Baidu, которая сочетает в себе отличные результаты модели и производительность вывода, превосходя ERNIE Lite, подходит для использования на AI-ускорителях с низкой вычислительной мощностью."
  },
  "ernie-novel-8k": {
    "description": "Универсальная большая языковая модель, разработанная Baidu, обладающая явными преимуществами в способности продолжать написание романов, также может использоваться в сценариях коротких пьес и фильмов."
  },
  "ernie-speed-128k": {
    "description": "Новая высокопроизводительная большая языковая модель, разработанная Baidu в 2024 году, обладающая выдающимися универсальными способностями, подходит для использования в качестве базовой модели для тонкой настройки, лучше справляясь с проблемами конкретных сценариев, при этом обладая отличной производительностью вывода."
  },
  "ernie-speed-pro-128k": {
    "description": "Новая высокопроизводительная большая языковая модель, разработанная Baidu в 2024 году, обладающая выдающимися универсальными способностями, превосходя ERNIE Speed, подходит для использования в качестве базовой модели для тонкой настройки, лучше справляясь с проблемами конкретных сценариев, при этом обладая отличной производительностью вывода."
  },
  "ernie-tiny-8k": {
    "description": "ERNIE Tiny — это сверхвысокопроизводительная большая языковая модель, стоимость развертывания и тонкой настройки которой является самой низкой среди моделей серии Wenxin."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Тюнинг) предлагает стабильную и настраиваемую производительность, что делает её идеальным выбором для решения сложных задач."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Тюнинг) предлагает выдающуюся поддержку многомодальности, сосредотачиваясь на эффективном решении сложных задач."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro — это высокопроизводительная модель ИИ от Google, разработанная для масштабирования широкого спектра задач."
  },
  "gemini-1.5-flash": {
    "description": "Gemini 1.5 Flash — это последняя многомодальная модель ИИ от Google, обладающая высокой скоростью обработки и поддерживающая текстовые, графические и видеовходы, что делает её эффективной для масштабирования различных задач."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 — это эффективная многомодальная модель, поддерживающая масштабирование для широкого спектра приложений."
  },
  "gemini-1.5-flash-002": {
    "description": "Gemini 1.5 Flash 002 — это эффективная мультимодальная модель, поддерживающая расширенные применения."
  },
  "gemini-1.5-flash-8b": {
    "description": "Gemini 1.5 Flash 8B — это высокоэффективная многомодальная модель, поддерживающая широкий спектр приложений."
  },
  "gemini-1.5-flash-8b-exp-0924": {
    "description": "Gemini 1.5 Flash 8B 0924 — это последняя экспериментальная модель, которая демонстрирует значительное улучшение производительности как в текстовых, так и в мультимодальных задачах."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 предлагает оптимизированные многомодальные возможности обработки, подходящие для различных сложных задач."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash — это последняя многомодальная модель ИИ от Google, обладающая высокой скоростью обработки и поддерживающая текстовые, графические и видео входы, что делает её эффективной для масштабирования различных задач."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 — это масштабируемое решение для многомодального ИИ, поддерживающее широкий спектр сложных задач."
  },
  "gemini-1.5-pro-002": {
    "description": "Gemini 1.5 Pro 002 — это последняя модель, готовая к производству, которая обеспечивает более высокое качество вывода, особенно в математических задачах, длинных контекстах и визуальных задачах."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 предлагает выдающиеся многомодальные возможности обработки, обеспечивая большую гибкость в разработке приложений."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 сочетает последние технологии оптимизации, обеспечивая более эффективную обработку многомодальных данных."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro поддерживает до 2 миллионов токенов и является идеальным выбором для средних многомодальных моделей, обеспечивая многостороннюю поддержку для сложных задач."
  },
  "gemini-2.0-flash": {
    "description": "Gemini 2.0 Flash предлагает функции следующего поколения и улучшения, включая выдающуюся скорость, использование встроенных инструментов, многомодальную генерацию и контекстное окно на 1M токенов."
  },
  "gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash предлагает функции следующего поколения и улучшения, включая выдающуюся скорость, использование встроенных инструментов, многомодальную генерацию и контекстное окно на 1M токенов."
  },
  "gemini-2.0-flash-lite-preview-02-05": {
    "description": "Модель Gemini 2.0 Flash, оптимизированная для экономической эффективности и низкой задержки."
  },
  "gemini-2.0-flash-thinking-exp-01-21": {
    "description": "Gemini 2.0 Flash Exp — это последняя экспериментальная многомодальная AI модель от Google, обладающая следующими поколениями характеристик, выдающейся скоростью, нативным вызовом инструментов и многомодальной генерацией."
  },
  "gemini-2.0-pro-exp-02-05": {
    "description": "Gemini 2.0 Pro Experimental — это последняя экспериментальная многомодальная AI модель от Google, которая демонстрирует определенное улучшение качества по сравнению с предыдущими версиями, особенно в области мировых знаний, кода и длинного контекста."
  },
  "gemma-7b-it": {
    "description": "Gemma 7B подходит для обработки задач среднего и малого масштаба, обеспечивая экономическую эффективность."
  },
  "gemma2": {
    "description": "Gemma 2 — это высокоэффективная модель, выпущенная Google, охватывающая широкий спектр приложений от малых до сложных задач обработки данных."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B — это модель, оптимизированная для конкретных задач и интеграции инструментов."
  },
  "gemma2:27b": {
    "description": "Gemma 2 — это высокоэффективная модель, выпущенная Google, охватывающая широкий спектр приложений от малых до сложных задач обработки данных."
  },
  "gemma2:2b": {
    "description": "Gemma 2 — это высокоэффективная модель, выпущенная Google, охватывающая широкий спектр приложений от малых до сложных задач обработки данных."
  },
  "generalv3": {
    "description": "Spark Pro — это высокопроизводительная большая языковая модель, оптимизированная для профессиональных областей, таких как математика, программирование, медицина и образование, поддерживающая сетевой поиск и встроенные плагины для погоды, даты и т.д. Оптимизированная модель демонстрирует выдающиеся результаты и высокую эффективность в сложных задачах на знание, понимании языка и высокоуровневом создании текстов, что делает ее идеальным выбором для профессиональных приложений."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max — это самая полная версия, поддерживающая сетевой поиск и множество встроенных плагинов. Его полностью оптимизированные основные возможности, а также функции настройки системных ролей и вызовов функций делают его выдающимся и эффективным в различных сложных приложениях."
  },
  "glm-4": {
    "description": "GLM-4 — это старая флагманская версия, выпущенная в январе 2024 года, которая была заменена более мощной GLM-4-0520."
  },
  "glm-4-0520": {
    "description": "GLM-4-0520 — это последняя версия модели, специально разработанная для высоко сложных и разнообразных задач, демонстрирующая выдающиеся результаты."
  },
  "glm-4-9b-chat": {
    "description": "GLM-4-9B-Chat демонстрирует высокую производительность в семантике, математике, логическом мышлении, кодировании и знаниях. Также поддерживает веб-браузинг, выполнение кода, вызовы пользовательских инструментов и длинное текстовое рассуждение. Поддерживает 26 языков, включая японский, корейский и немецкий."
  },
  "glm-4-air": {
    "description": "GLM-4-Air — это экономически эффективная версия, производительность которой близка к GLM-4, обеспечивая высокую скорость и доступную цену."
  },
  "glm-4-airx": {
    "description": "GLM-4-AirX предлагает эффективную версию GLM-4-Air, скорость вывода может достигать 2.6 раз быстрее."
  },
  "glm-4-alltools": {
    "description": "GLM-4-AllTools — это многофункциональная модель агента, оптимизированная для поддержки сложного планирования инструкций и вызовов инструментов, таких как веб-серфинг, интерпретация кода и генерация текста, подходящая для выполнения множества задач."
  },
  "glm-4-flash": {
    "description": "GLM-4-Flash — это идеальный выбор для обработки простых задач, с самой высокой скоростью и самой низкой ценой."
  },
  "glm-4-flashx": {
    "description": "GLM-4-FlashX — это улучшенная версия Flash с ультрабыстрой скоростью вывода."
  },
  "glm-4-long": {
    "description": "GLM-4-Long поддерживает сверхдлинные текстовые вводы, подходит для задач, требующих памяти, и обработки больших документов."
  },
  "glm-4-plus": {
    "description": "GLM-4-Plus, как флагман с высоким интеллектом, обладает мощными способностями обработки длинных текстов и сложных задач, с полным улучшением производительности."
  },
  "glm-4v": {
    "description": "GLM-4V предлагает мощные способности понимания и вывода изображений, поддерживает множество визуальных задач."
  },
  "glm-4v-flash": {
    "description": "GLM-4V-Flash сосредоточен на эффективном понимании одного изображения, подходит для сценариев быстрого анализа изображений, таких как анализ изображений в реальном времени или пакетная обработка изображений."
  },
  "glm-4v-plus": {
    "description": "GLM-4V-Plus обладает способностью понимать видео-контент и множество изображений, подходит для мультимодальных задач."
  },
  "glm-zero-preview": {
    "description": "GLM-Zero-Preview обладает мощными способностями к сложному выводу, демонстрируя отличные результаты в области логического вывода, математики и программирования."
  },
  "google/gemini-2.0-flash-001": {
    "description": "Gemini 2.0 Flash предлагает функции следующего поколения и улучшения, включая выдающуюся скорость, использование встроенных инструментов, многомодальную генерацию и контекстное окно на 1M токенов."
  },
  "google/gemini-2.0-pro-exp-02-05:free": {
    "description": "Gemini 2.0 Pro Experimental — это последняя экспериментальная многомодальная AI модель от Google, которая демонстрирует определенное улучшение качества по сравнению с предыдущими версиями, особенно в области мировых знаний, кода и длинного контекста."
  },
  "google/gemini-flash-1.5": {
    "description": "Gemini 1.5 Flash предлагает оптимизированные возможности многомодальной обработки, подходящие для различных сложных задач."
  },
  "google/gemini-pro-1.5": {
    "description": "Gemini 1.5 Pro сочетает в себе новейшие технологии оптимизации, обеспечивая более эффективную обработку многомодальных данных."
  },
  "google/gemma-2-27b": {
    "description": "Gemma 2 — это эффективная модель, представленная Google, охватывающая широкий спектр приложений от небольших до сложных задач обработки данных."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 продолжает концепцию легковесного и эффективного дизайна."
  },
  "google/gemma-2-2b-it": {
    "description": "Легковесная модель настройки инструкций от Google."
  },
  "google/gemma-2-9b": {
    "description": "Gemma 2 — это эффективная модель, представленная Google, охватывающая широкий спектр приложений от небольших до сложных задач обработки данных."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 — это легковесная серия текстовых моделей с открытым исходным кодом от Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 — это облегченная открытая текстовая модель от Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) предлагает базовые возможности обработки команд, подходящие для легковесных приложений."
  },
  "gpt-3.5-turbo": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "GPT 3.5 Turbo подходит для различных задач генерации и понимания текста, в настоящее время ссылается на gpt-3.5-turbo-0125."
  },
  "gpt-35-turbo": {
    "description": "GPT 3.5 Turbo — это эффективная модель от OpenAI, предназначенная для задач чата и генерации текста, поддерживающая параллельные вызовы функций."
  },
  "gpt-35-turbo-16k": {
    "description": "GPT 3.5 Turbo 16k — модель для генерации текста с высокой ёмкостью, подходящая для сложных задач."
  },
  "gpt-4": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-0125-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-0613": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-1106-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-32k": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-32k-0613": {
    "description": "GPT-4 предлагает более широкий контекстный диапазон, способный обрабатывать более длинные текстовые вводы, подходя для сценариев, требующих обширной интеграции информации и анализа данных."
  },
  "gpt-4-turbo": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-turbo-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4-vision-preview": {
    "description": "Последняя модель GPT-4 Turbo обладает визуальными функциями. Теперь визуальные запросы могут использовать JSON-формат и вызовы функций. GPT-4 Turbo — это улучшенная версия, обеспечивающая экономически эффективную поддержку для мультимодальных задач. Она находит баланс между точностью и эффективностью, подходя для приложений, требующих взаимодействия в реальном времени."
  },
  "gpt-4o": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-2024-05-13": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощное понимание языка и генерацию, подходя для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-2024-11-20": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени для поддержания актуальной версии. Она сочетает в себе мощное понимание языка и генерацию текста, подходя для широкого спектра приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "gpt-4o-audio-preview": {
    "description": "Модель GPT-4o Audio, поддерживающая аудиовход и аудиовыход."
  },
  "gpt-4o-mini": {
    "description": "GPT-4o mini — это последняя модель, выпущенная OpenAI после GPT-4 Omni, поддерживающая ввод изображений и текстов с выводом текста. Как их самый продвинутый компактный модель, она значительно дешевле других недавних передовых моделей и более чем на 60% дешевле GPT-3.5 Turbo. Она сохраняет передовой уровень интеллекта при значительном соотношении цена-качество. GPT-4o mini набрала 82% на тесте MMLU и в настоящее время занимает более высокое место в предпочтениях чата по сравнению с GPT-4."
  },
  "gpt-4o-mini-realtime-preview": {
    "description": "Реальная версия GPT-4o-mini, поддерживающая аудио и текстовый ввод и вывод в реальном времени."
  },
  "gpt-4o-realtime-preview": {
    "description": "Реальная версия GPT-4o, поддерживающая аудио и текстовый ввод и вывод в реальном времени."
  },
  "gpt-4o-realtime-preview-2024-10-01": {
    "description": "Реальная версия GPT-4o, поддерживающая аудио и текстовый ввод и вывод в реальном времени."
  },
  "gpt-4o-realtime-preview-2024-12-17": {
    "description": "Реальная версия GPT-4o, поддерживающая аудио и текстовый ввод и вывод в реальном времени."
  },
  "grok-2-1212": {
    "description": "Модель улучшена в точности, соблюдении инструкций и многоязычных возможностях."
  },
  "grok-2-vision-1212": {
    "description": "Модель улучшена в точности, соблюдении инструкций и многоязычных возможностях."
  },
  "grok-beta": {
    "description": "Обладает производительностью, сопоставимой с Grok 2, но с большей эффективностью, скоростью и функциональностью."
  },
  "grok-vision-beta": {
    "description": "Новейшая модель понимания изображений, способная обрабатывать разнообразную визуальную информацию, включая документы, графики, скриншоты и фотографии."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B — это языковая модель, объединяющая креативность и интеллект, основанная на нескольких ведущих моделях."
  },
  "hunyuan-code": {
    "description": "Последняя модель генерации кода Hunyuan, обученная на базе 200B высококачественных данных кода, прошедшая полгода обучения на высококачественных данных SFT, с увеличенной длиной контекстного окна до 8K, занимает ведущие позиции по автоматическим оценочным показателям генерации кода на пяти языках; по десяти критериям оценки кода на пяти языках, производительность находится в первой группе."
  },
  "hunyuan-functioncall": {
    "description": "Последняя модель Hunyuan с архитектурой MOE FunctionCall, обученная на высококачественных данных FunctionCall, с контекстным окном до 32K, занимает лидирующие позиции по множеству оценочных показателей."
  },
  "hunyuan-large": {
    "description": "Модель Hunyuan-large имеет общее количество параметров около 389B, активных параметров около 52B, что делает её самой крупной и эффективной открытой моделью MoE с архитектурой Transformer в отрасли."
  },
  "hunyuan-large-longcontext": {
    "description": "Специализируется на обработке длинных текстовых задач, таких как резюме документов и вопросы и ответы по документам, а также обладает способностью обрабатывать общие задачи генерации текста. Отлично справляется с анализом и генерацией длинных текстов, эффективно справляясь с требованиями к обработке сложного и детального длинного контента."
  },
  "hunyuan-lite": {
    "description": "Обновленная версия с MOE-структурой, контекстное окно составляет 256k, она опережает множество открытых моделей в оценках по NLP, коду, математике и другим областям."
  },
  "hunyuan-lite-vision": {
    "description": "Последняя многомодальная модель Hunyuan с 7B параметрами, окно контекста 32K, поддерживает многомодальный диалог на китайском и английском языках, распознавание объектов на изображениях, понимание документов и таблиц, многомодальную математику и т. д., по многим измерениям превосходит модели конкурентов с 7B параметрами."
  },
  "hunyuan-pro": {
    "description": "Модель длинного текста с параметрами уровня триллиона MOE-32K. Она достигает абсолютного лидерства на различных бенчмарках, обладает сложными инструкциями и выводом, имеет сложные математические способности и поддерживает вызовы функций, с акцентом на оптимизацию в области многоязычного перевода, финансов, права и медицины."
  },
  "hunyuan-role": {
    "description": "Последняя версия модели ролевого взаимодействия Hunyuan, выпущенная с официальной тонкой настройкой, основанная на модели Hunyuan и дополненная данными сценариев ролевого взаимодействия, демонстрирует лучшие базовые результаты в ролевых сценариях."
  },
  "hunyuan-standard": {
    "description": "Использует более оптимальную стратегию маршрутизации, одновременно смягчая проблемы с балансировкой нагрузки и сходимостью экспертов. В области длинных текстов показатель «найти иголку в стоге сена» достигает 99,9%. MOE-32K предлагает более высокую стоимость-эффективность, обеспечивая баланс между качеством и ценой, а также возможность обработки длинных текстовых вводов."
  },
  "hunyuan-standard-256K": {
    "description": "Использует более оптимальную стратегию маршрутизации, одновременно смягчая проблемы с балансировкой нагрузки и сходимостью экспертов. В области длинных текстов показатель «найти иголку в стоге сена» достигает 99,9%. MOE-256K делает дальнейший прорыв в длине и качестве, значительно расширяя допустимую длину ввода."
  },
  "hunyuan-standard-vision": {
    "description": "Последняя многомодальная модель Hunyuan, поддерживающая многоязычные ответы, с сбалансированными способностями на китайском и английском языках."
  },
  "hunyuan-translation": {
    "description": "Поддерживает взаимный перевод на 15 языков, включая китайский, английский, японский, французский, португальский, испанский, турецкий, русский, арабский, корейский, итальянский, немецкий, вьетнамский, малайский и индонезийский, с автоматической оценкой на основе набора тестов для многофункционального перевода COMET, в целом превосходя модели аналогичного масштаба на рынке по способности к взаимному переводу среди более чем десяти распространенных языков."
  },
  "hunyuan-translation-lite": {
    "description": "Модель перевода Хуньюань поддерживает перевод в формате естественного языкового диалога; поддерживает взаимный перевод на 15 языков, включая китайский, английский, японский, французский, португальский, испанский, турецкий, русский, арабский, корейский, итальянский, немецкий, вьетнамский, малайский и индонезийский."
  },
  "hunyuan-turbo": {
    "description": "Предварительная версия нового поколения языковой модели Hunyuan, использующая совершенно новую структуру смешанной экспертной модели (MoE), которая обеспечивает более быструю эффективность вывода и более сильные результаты по сравнению с hunyuan-pro."
  },
  "hunyuan-turbo-20241120": {
    "description": "Фиксированная версия hunyuan-turbo от 20 ноября 2024 года, промежуточная между hunyuan-turbo и hunyuan-turbo-latest."
  },
  "hunyuan-turbo-20241223": {
    "description": "Оптимизация этой версии: масштабирование данных и инструкций, значительное повышение общей обобщающей способности модели; значительное улучшение математических, кодовых и логических способностей; оптимизация понимания текста и связанных с ним способностей понимания слов; оптимизация качества генерации контента при создании текста."
  },
  "hunyuan-turbo-latest": {
    "description": "Оптимизация общего опыта, включая понимание NLP, создание текста, общение, вопросы и ответы на знания, перевод, области и т. д.; повышение человечности, оптимизация эмоционального интеллекта модели; улучшение способности модели активно прояснять неясные намерения; повышение способности обработки вопросов, связанных с анализом слов; улучшение качества и интерактивности творчества; улучшение многократного взаимодействия."
  },
  "hunyuan-turbo-vision": {
    "description": "Флагманская модель нового поколения Hunyuan в области визуального языка, использующая совершенно новую структуру смешанной экспертной модели (MoE), с полным улучшением способностей в области базового распознавания, создания контента, вопросов и ответов на знания, анализа и вывода по сравнению с предыдущей моделью."
  },
  "hunyuan-vision": {
    "description": "Последняя многомодальная модель Hunyuan, поддерживающая ввод изображений и текста для генерации текстового контента."
  },
  "internlm/internlm2_5-20b-chat": {
    "description": "Инновационная открытая модель InternLM2.5, благодаря большому количеству параметров, повышает интеллектуальность диалогов."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 предлагает интеллектуальные решения для диалогов в различных сценариях."
  },
  "internlm2-pro-chat": {
    "description": "Старая версия модели, которую мы все еще поддерживаем, доступная с параметрами 7B и 20B."
  },
  "internlm2.5-latest": {
    "description": "Наша последняя серия моделей с выдающимися показателями вывода, поддерживающая длину контекста до 1M и обладающая улучшенными возможностями следования инструкциям и вызова инструментов."
  },
  "internlm3-latest": {
    "description": "Наша последняя серия моделей с выдающейся производительностью вывода, лидирующая среди моделей открытого кода того же уровня. По умолчанию указывает на нашу последнюю выпущенную серию моделей InternLM3."
  },
  "jina-deepsearch-v1": {
    "description": "Глубокий поиск сочетает в себе сетевой поиск, чтение и рассуждение, позволяя проводить всесторонние исследования. Вы можете рассматривать его как агента, который принимает ваши исследовательские задачи — он проводит обширный поиск и проходит через множество итераций, прежде чем предоставить ответ. Этот процесс включает в себя постоянные исследования, рассуждения и решение проблем с разных точек зрения. Это принципиально отличается от стандартных больших моделей, которые генерируют ответы непосредственно из предобученных данных, и от традиционных систем RAG, полагающихся на одноразовый поверхностный поиск."
  },
  "kimi-latest": {
    "description": "Продукт Kimi Smart Assistant использует последнюю модель Kimi, которая может содержать нестабильные функции. Поддерживает понимание изображений и автоматически выбирает модель 8k/32k/128k в качестве модели для выставления счетов в зависимости от длины контекста запроса."
  },
  "learnlm-1.5-pro-experimental": {
    "description": "LearnLM — это экспериментальная языковая модель, ориентированная на конкретные задачи, обученная в соответствии с принципами науки о обучении, которая может следовать системным инструкциям в учебных и образовательных сценариях, выступая в роли эксперта-наставника и т.д."
  },
  "lite": {
    "description": "Spark Lite — это легковесная большая языковая модель с крайне низкой задержкой и высокой эффективностью обработки, полностью бесплатная и открытая, поддерживающая функции онлайн-поиска в реальном времени. Ее быстрая реакция делает ее отличным выбором для применения в устройствах с низкой вычислительной мощностью и для тонкой настройки моделей, обеспечивая пользователям отличное соотношение цены и качества, особенно в сценариях вопросов и ответов, генерации контента и поиска."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B предлагает более мощные возможности ИИ вывода, подходит для сложных приложений, поддерживает огромное количество вычислительных процессов и гарантирует эффективность и точность."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B — это высокоэффективная модель, обеспечивающая быструю генерацию текста, идеально подходящая для приложений, требующих масштабной эффективности и экономичности."
  },
  "llama-3.1-sonar-huge-128k-online": {
    "description": "Модель Llama 3.1 Sonar Huge Online, обладающая 405B параметрами, поддерживает контекст длиной около 127,000 токенов, предназначена для сложных онлайн-чат-приложений."
  },
  "llama-3.1-sonar-large-128k-online": {
    "description": "Модель Llama 3.1 Sonar Large Online, обладающая 70B параметрами, поддерживает контекст длиной около 127,000 токенов, подходит для задач с высокой нагрузкой и разнообразными чатами."
  },
  "llama-3.1-sonar-small-128k-online": {
    "description": "Модель Llama 3.1 Sonar Small Online, обладающая 8B параметрами, поддерживает контекст длиной около 127,000 токенов, специально разработана для онлайн-чатов и эффективно обрабатывает различные текстовые взаимодействия."
  },
  "llama-3.2-11b-vision-instruct": {
    "description": "Отличные способности к визуальному пониманию изображений на высоком разрешении, предназначенные для приложений визуального понимания."
  },
  "llama-3.2-11b-vision-preview": {
    "description": "Llama 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "llama-3.2-90b-vision-instruct": {
    "description": "Совершенные возможности визуального понимания для приложения-агента."
  },
  "llama-3.2-90b-vision-preview": {
    "description": "Llama 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "llama-3.3-70b-instruct": {
    "description": "Llama 3.3 — это самая современная многоязычная открытая языковая модель из серии Llama, которая позволяет получить производительность, сопоставимую с 405B моделями, по очень низкой цене. Основана на структуре Transformer и улучшена с помощью контролируемой донастройки (SFT) и обучения с подкреплением на основе человеческой обратной связи (RLHF) для повышения полезности и безопасности. Ее версия с оптимизацией под инструкции специально разработана для многоязычных диалогов и показывает лучшие результаты по сравнению с множеством открытых и закрытых моделей чата на различных отраслевых бенчмарках. Дата окончания знаний — декабрь 2023 года."
  },
  "llama-3.3-70b-versatile": {
    "description": "Многоязычная большая языковая модель Meta Llama 3.3 (LLM) — это предобученная и откорректированная модель генерации на 70B (текстовый ввод/текстовый вывод). Откорректированная на чистом тексте модель Llama 3.3 оптимизирована для многоязычных диалоговых задач и превосходит многие доступные открытые и закрытые модели чата по общим промышленным стандартам."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B предлагает непревзойдённые возможности обработки сложности, специально разработанные для высоких требований проектов."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B обеспечивает высококачественную производительность вывода, подходящую для многообразных приложений."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use предлагает мощные возможности вызова инструментов, поддерживая эффективную обработку сложных задач."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use — это модель, оптимизированная для эффективного использования инструментов, поддерживающая быструю параллельную обработку."
  },
  "llama3.1": {
    "description": "Llama 3.1 — это передовая модель, выпущенная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 — это передовая модель, выпущенная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 — это передовая модель, выпущенная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "llava": {
    "description": "LLaVA — это многомодальная модель, объединяющая визуальный кодировщик и Vicuna, предназначенная для мощного понимания визуальной и языковой информации."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B предлагает возможности визуальной обработки, генерируя сложные выходные данные на основе визуальной информации."
  },
  "llava:13b": {
    "description": "LLaVA — это многомодальная модель, объединяющая визуальный кодировщик и Vicuna, предназначенная для мощного понимания визуальной и языковой информации."
  },
  "llava:34b": {
    "description": "LLaVA — это многомодальная модель, объединяющая визуальный кодировщик и Vicuna, предназначенная для мощного понимания визуальной и языковой информации."
  },
  "mathstral": {
    "description": "MathΣtral специально разработан для научных исследований и математического вывода, обеспечивая эффективные вычислительные возможности и интерпретацию результатов."
  },
  "max-32k": {
    "description": "Spark Max 32K обладает большой способностью обработки контекста, улучшенным пониманием контекста и логическим выводом, поддерживает текстовый ввод до 32K токенов, подходит для чтения длинных документов, частных вопросов и ответов и других сценариев."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Мощная модель с 70 миллиардами параметров, превосходящая в области рассуждений, кодирования и широких языковых приложений."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Универсальная модель с 8 миллиардами параметров, оптимизированная для диалоговых и текстовых задач."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Модели Llama 3.1, настроенные на инструкции, оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные модели открытого и закрытого чата по общим отраслевым стандартам."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Модели Llama 3.1, настроенные на инструкции, оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные модели открытого и закрытого чата по общим отраслевым стандартам."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Модели Llama 3.1, настроенные на инструкции, оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные модели открытого и закрытого чата по общим отраслевым стандартам."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) предлагает отличные возможности обработки языка и выдающийся опыт взаимодействия."
  },
  "meta-llama/Llama-2-70b-hf": {
    "description": "LLaMA-2 предлагает превосходные способности обработки языка и выдающийся пользовательский опыт."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) — мощная модель для чата, поддерживающая сложные диалоговые запросы."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) предлагает многоязычную поддержку и охватывает широкий спектр областей знаний."
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Llama-3.2-3B-Instruct-Turbo": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Llama-3.3-70B-Instruct": {
    "description": "Llama 3.3 — это самая современная многоязычная открытая языковая модель серии Llama, позволяющая получить производительность, сопоставимую с 405B моделью, по очень низкой цене. Основана на структуре Transformer и улучшена с помощью контролируемой донастройки (SFT) и обучения с подкреплением на основе человеческой обратной связи (RLHF) для повышения полезности и безопасности. Ее версия с оптимизацией под инструкции специально разработана для многоязычного диалога и показывает лучшие результаты по сравнению с многими открытыми и закрытыми чат-моделями на нескольких отраслевых бенчмарках. Дата окончания знаний — декабрь 2023 года."
  },
  "meta-llama/Llama-3.3-70B-Instruct-Turbo": {
    "description": "Многоязычная большая языковая модель Meta Llama 3.3 (LLM) — это предобученная и настроенная на инструкции генеративная модель объемом 70B (входной/выходной текст). Модель Llama 3.3, настроенная на инструкции, оптимизирована для многоязычных диалоговых случаев и превосходит многие доступные открытые и закрытые модели чата по общим отраслевым бенчмаркам."
  },
  "meta-llama/Llama-Vision-Free": {
    "description": "LLaMA 3.2 предназначена для выполнения задач, объединяющих визуальные и текстовые данные. Она отлично справляется с задачами по описанию изображений и визуальному вопросу-ответу, преодолевая разрыв между генерацией языка и визуальным пониманием."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite подходит для сред, требующих высокой производительности и низкой задержки."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo обеспечивает выдающиеся возможности понимания и генерации языка, подходящие для самых требовательных вычислительных задач."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite подходит для ресурсов ограниченных сред, обеспечивая отличное соотношение производительности."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo — это высокоэффективная большая языковая модель, поддерживающая широкий спектр приложений."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B — это мощная модель, основанная на предобучении и настройке инструкций."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "Модель Llama 3.1 Turbo 405B предлагает огромную поддержку контекста для обработки больших данных и демонстрирует выдающиеся результаты в масштабных приложениях искусственного интеллекта."
  },
  "meta-llama/Meta-Llama-3.1-70B": {
    "description": "Llama 3.1 — это передовая модель, представленная Meta, поддерживающая до 405B параметров, применимая в сложных диалогах, многоязычном переводе и анализе данных."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct": {
    "description": "LLaMA 3.1 70B предлагает эффективную поддержку диалогов на нескольких языках."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "Модель Llama 3.1 70B была тщательно настроена для высоконагруженных приложений, квантованная до FP8 для повышения вычислительной мощности и точности, обеспечивая выдающиеся результаты в сложных сценариях."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "LLaMA 3.1 предлагает поддержку нескольких языков и является одной из ведущих генеративных моделей в отрасли."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "Модель Llama 3.1 8B использует FP8-квантование и поддерживает до 131,072 контекстных токенов, являясь выдающейся среди открытых моделей, подходящей для сложных задач и превосходящей многие отраслевые стандарты."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct оптимизирован для высококачественных диалоговых сцен и показывает отличные результаты в различных оценках."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct оптимизирован для высококачественных диалоговых сцен, его производительность превосходит многие закрытые модели."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct разработан для высококачественных диалогов и показывает выдающиеся результаты в оценках, особенно в высокоинтерактивных сценах."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct — это последняя версия от Meta, оптимизированная для высококачественных диалоговых сцен, превосходящая многие ведущие закрытые модели."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 предлагает поддержку нескольких языков и является одной из ведущих генеративных моделей в отрасли."
  },
  "meta-llama/llama-3.2-11b-vision-instruct": {
    "description": "LLaMA 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "meta-llama/llama-3.2-90b-vision-instruct": {
    "description": "LLaMA 3.2 предназначена для обработки задач, сочетающих визуальные и текстовые данные. Она демонстрирует отличные результаты в задачах описания изображений и визуального вопросно-ответного взаимодействия, преодолевая разрыв между генерацией языка и визуальным выводом."
  },
  "meta-llama/llama-3.3-70b-instruct": {
    "description": "Llama 3.3 — это самая современная многоязычная открытая языковая модель из серии Llama, которая позволяет получить производительность, сопоставимую с 405B моделями, по очень низкой цене. Основана на структуре Transformer и улучшена с помощью контролируемой донастройки (SFT) и обучения с подкреплением на основе человеческой обратной связи (RLHF) для повышения полезности и безопасности. Ее версия с оптимизацией под инструкции специально разработана для многоязычных диалогов и показывает лучшие результаты по сравнению с множеством открытых и закрытых моделей чата на различных отраслевых бенчмарках. Дата окончания знаний — декабрь 2023 года."
  },
  "meta-llama/llama-3.3-70b-instruct:free": {
    "description": "Llama 3.3 — это самая современная многоязычная открытая языковая модель из серии Llama, которая позволяет получить производительность, сопоставимую с 405B моделями, по очень низкой цене. Основана на структуре Transformer и улучшена с помощью контролируемой донастройки (SFT) и обучения с подкреплением на основе человеческой обратной связи (RLHF) для повышения полезности и безопасности. Ее версия с оптимизацией под инструкции специально разработана для многоязычных диалогов и показывает лучшие результаты по сравнению с множеством открытых и закрытых моделей чата на различных отраслевых бенчмарках. Дата окончания знаний — декабрь 2023 года."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct — это самая большая и мощная модель в линейке Llama 3.1 Instruct, представляющая собой высокоразвёрнутую модель для диалогового вывода и генерации синтетических данных, также может использоваться в качестве основы для специализированного предобучения или дообучения в определённых областях. Многоязычные большие языковые модели (LLMs), предлагаемые Llama 3.1, представляют собой набор предобученных генеративных моделей с настройкой на инструкции, включая размеры 8B, 70B и 405B (вход/выход текста). Модели текста с настройкой на инструкции Llama 3.1 (8B, 70B, 405B) оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные открытые модели чата в общепринятых отраслевых бенчмарках. Llama 3.1 предназначена для коммерческого и исследовательского использования на нескольких языках. Модели текста с настройкой на инструкции подходят для диалогов, похожих на помощников, в то время как предобученные модели могут адаптироваться к различным задачам генерации естественного языка. Модели Llama 3.1 также поддерживают использование их вывода для улучшения других моделей, включая генерацию синтетических данных и уточнение. Llama 3.1 является саморегрессионной языковой моделью, использующей оптимизированную архитектуру трансформеров. Настроенные версии используют контролируемое дообучение (SFT) и обучение с подкреплением с человеческой обратной связью (RLHF), чтобы соответствовать предпочтениям людей в отношении полезности и безопасности."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "Обновленная версия Meta Llama 3.1 70B Instruct, включающая расширенную длину контекста до 128K, многоязычность и улучшенные способности вывода. Многоязычные большие языковые модели (LLMs), предлагаемые Llama 3.1, представляют собой набор предобученных, настроенных на инструкции генеративных моделей, включая размеры 8B, 70B и 405B (ввод/вывод текста). Настроенные на инструкции текстовые модели (8B, 70B, 405B) оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные открытые модели чата в общих отраслевых бенчмарках. Llama 3.1 предназначена для коммерческого и исследовательского использования на нескольких языках. Настроенные на инструкции текстовые модели подходят для диалогов, похожих на помощника, в то время как предобученные модели могут адаптироваться к различным задачам генерации естественного языка. Модели Llama 3.1 также поддерживают использование вывода своих моделей для улучшения других моделей, включая генерацию синтетических данных и уточнение. Llama 3.1 — это саморегрессионная языковая модель, использующая оптимизированную архитектуру трансформеров. Настроенные версии используют контролируемую донастройку (SFT) и обучение с подкреплением с человеческой обратной связью (RLHF), чтобы соответствовать человеческим предпочтениям по полезности и безопасности."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "Обновленная версия Meta Llama 3.1 8B Instruct, включающая расширенную длину контекста до 128K, многоязычность и улучшенные способности вывода. Многоязычные большие языковые модели (LLMs), предлагаемые Llama 3.1, представляют собой набор предобученных, настроенных на инструкции генеративных моделей, включая размеры 8B, 70B и 405B (ввод/вывод текста). Настроенные на инструкции текстовые модели (8B, 70B, 405B) оптимизированы для многоязычных диалоговых случаев и превосходят многие доступные открытые модели чата в общих отраслевых бенчмарках. Llama 3.1 предназначена для коммерческого и исследовательского использования на нескольких языках. Настроенные на инструкции текстовые модели подходят для диалогов, похожих на помощника, в то время как предобученные модели могут адаптироваться к различным задачам генерации естественного языка. Модели Llama 3.1 также поддерживают использование вывода своих моделей для улучшения других моделей, включая генерацию синтетических данных и уточнение. Llama 3.1 — это саморегрессионная языковая модель, использующая оптимизированную архитектуру трансформеров. Настроенные версии используют контролируемую донастройку (SFT) и обучение с подкреплением с человеческой обратной связью (RLHF), чтобы соответствовать человеческим предпочтениям по полезности и безопасности."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 — это открытая большая языковая модель (LLM), ориентированная на разработчиков, исследователей и предприятия, предназначенная для помощи в создании, экспериментировании и ответственном масштабировании их идей по генеративному ИИ. В качестве части базовой системы для инноваций глобального сообщества она идеально подходит для создания контента, диалогового ИИ, понимания языка, НИОКР и корпоративных приложений."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 — это открытая большая языковая модель (LLM), ориентированная на разработчиков, исследователей и предприятия, предназначенная для помощи в создании, экспериментировании и ответственном масштабировании их идей по генеративному ИИ. В качестве части базовой системы для инноваций глобального сообщества она идеально подходит для устройств с ограниченными вычислительными мощностями и ресурсами, а также для более быстрого времени обучения."
  },
  "meta/llama-3.1-405b-instruct": {
    "description": "Современная LLM, поддерживающая генерацию синтетических данных, дистилляцию знаний и рассуждения, подходит для чат-ботов, программирования и специализированных задач."
  },
  "meta/llama-3.1-70b-instruct": {
    "description": "Обеспечивает сложные диалоги, обладая выдающимся пониманием контекста, способностями к рассуждению и генерации текста."
  },
  "meta/llama-3.1-8b-instruct": {
    "description": "Современная передовая модель, обладающая пониманием языка, выдающимися способностями к рассуждению и генерации текста."
  },
  "meta/llama-3.2-11b-vision-instruct": {
    "description": "Современная визуально-языковая модель, специализирующаяся на высококачественном рассуждении на основе изображений."
  },
  "meta/llama-3.2-1b-instruct": {
    "description": "Современная передовая компактная языковая модель, обладающая пониманием языка, выдающимися способностями к рассуждению и генерации текста."
  },
  "meta/llama-3.2-3b-instruct": {
    "description": "Современная передовая компактная языковая модель, обладающая пониманием языка, выдающимися способностями к рассуждению и генерации текста."
  },
  "meta/llama-3.2-90b-vision-instruct": {
    "description": "Современная визуально-языковая модель, специализирующаяся на высококачественном рассуждении на основе изображений."
  },
  "meta/llama-3.3-70b-instruct": {
    "description": "Современная LLM, специализирующаяся на рассуждениях, математике, здравом смысле и вызовах функций."
  },
  "microsoft/WizardLM-2-8x22B": {
    "description": "WizardLM 2 — это языковая модель от Microsoft AI, которая особенно хорошо справляется с сложными диалогами, многоязычностью, выводами и интеллектуальными помощниками."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B — это передовая модель Wizard от Microsoft, демонстрирующая исключительно конкурентоспособные результаты."
  },
  "minicpm-v": {
    "description": "MiniCPM-V — это новое поколение мультимодальной большой модели от OpenBMB, обладающее выдающимися возможностями OCR и мультимодального понимания, поддерживающее широкий спектр приложений."
  },
  "ministral-3b-latest": {
    "description": "Ministral 3B - это выдающаяся модель от Mistral."
  },
  "ministral-8b-latest": {
    "description": "Ministral 8B - это экономически эффективная модель от Mistral."
  },
  "mistral": {
    "description": "Mistral — это 7B модель, выпущенная Mistral AI, подходящая для разнообразных языковых задач."
  },
  "mistral-large": {
    "description": "Mixtral Large — это флагманская модель от Mistral, объединяющая возможности генерации кода, математики и вывода, поддерживающая контекстное окно 128k."
  },
  "mistral-large-latest": {
    "description": "Mistral Large — это флагманская большая модель, хорошо подходящая для многоязычных задач, сложного вывода и генерации кода, идеальный выбор для высококлассных приложений."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo, разработанный в сотрудничестве между Mistral AI и NVIDIA, является высокоэффективной 12B моделью."
  },
  "mistral-small": {
    "description": "Mistral Small может использоваться для любых языковых задач, требующих высокой эффективности и низкой задержки."
  },
  "mistral-small-latest": {
    "description": "Mistral Small — это экономически эффективный, быстрый и надежный вариант для таких случаев, как перевод, резюме и анализ настроений."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct известен своей высокой производительностью и подходит для множества языковых задач."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B — это модель с настройкой по запросу, предлагающая оптимизированные ответы на задачи."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 обеспечивает эффективные вычислительные возможности и понимание естественного языка, подходящие для широкого спектра приложений."
  },
  "mistralai/Mistral-7B-v0.1": {
    "description": "Mistral 7B - это компактная, но высокопроизводительная модель, хорошо подходящая для пакетной обработки и простых задач, таких как классификация и генерация текста, с хорошими способностями к рассуждению."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) — это супер большая языковая модель, поддерживающая крайне высокие требования к обработке."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B — это предобученная модель разреженных смешанных экспертов, предназначенная для универсальных текстовых задач."
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "description": "Mixtral 8x7B - это разреженная модель эксперта, использующая множество параметров для повышения скорости вывода, подходит для обработки многоязычных и генеративных задач."
  },
  "mistralai/mistral-7b-instruct": {
    "description": "Mistral 7B Instruct — это высокопроизводительная модель стандартов отрасли, оптимизированная для скорости и поддержки длинного контекста."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo — это модель с 7.3B параметрами, поддерживающая несколько языков и высокопроизводительное программирование."
  },
  "mixtral": {
    "description": "Mixtral — это экспертная модель от Mistral AI, обладающая открытыми весами и поддерживающая генерацию кода и понимание языка."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B предлагает высокую отказоустойчивость параллельной обработки, подходящей для сложных задач."
  },
  "mixtral:8x22b": {
    "description": "Mixtral — это экспертная модель от Mistral AI, обладающая открытыми весами и поддерживающая генерацию кода и понимание языка."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K — это модель с возможностями обработки сверхдлинного контекста, подходящая для генерации очень длинных текстов, удовлетворяющая требованиям сложных задач генерации, способная обрабатывать до 128 000 токенов, идеально подходящая для научных исследований, академических и крупных документальных приложений."
  },
  "moonshot-v1-128k-vision-preview": {
    "description": "Модель визуализации Kimi (включая moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview и др.) может понимать содержимое изображений, включая текст на изображениях, цвета изображений и формы объектов."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K предлагает возможности обработки контекста средней длины, способная обрабатывать 32 768 токенов, особенно подходит для генерации различных длинных документов и сложных диалогов, применяется в создании контента, генерации отчетов и диалоговых систем."
  },
  "moonshot-v1-32k-vision-preview": {
    "description": "Модель визуализации Kimi (включая moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview и др.) может понимать содержимое изображений, включая текст на изображениях, цвета изображений и формы объектов."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K специально разработан для генерации коротких текстов, обладая высокой производительностью обработки, способный обрабатывать 8 192 токена, идеально подходит для кратких диалогов, стенографирования и быстрой генерации контента."
  },
  "moonshot-v1-8k-vision-preview": {
    "description": "Модель визуализации Kimi (включая moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview и др.) может понимать содержимое изображений, включая текст на изображениях, цвета изображений и формы объектов."
  },
  "moonshot-v1-auto": {
    "description": "Moonshot V1 Auto может выбирать подходящую модель в зависимости от количества токенов, используемых в текущем контексте."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B — это обновленная версия Nous Hermes 2, содержащая последние внутренние разработанные наборы данных."
  },
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
    "description": "Llama 3.1 Nemotron 70B — это крупная языковая модель, созданная NVIDIA, предназначенная для повышения полезности ответов, генерируемых LLM, на запросы пользователей. Эта модель показала отличные результаты в таких бенчмарках, как Arena Hard, AlpacaEval 2 LC и GPT-4-Turbo MT-Bench, и на 1 октября 2024 года занимает первое место во всех трех автоматических тестах на согласование. Модель обучалась с использованием RLHF (в частности, REINFORCE), Llama-3.1-Nemotron-70B-Reward и HelpSteer2-Preference на основе модели Llama-3.1-70B-Instruct."
  },
  "nvidia/llama-3.1-nemotron-51b-instruct": {
    "description": "Уникальная языковая модель, обеспечивающая непревзойденную точность и эффективность."
  },
  "nvidia/llama-3.1-nemotron-70b-instruct": {
    "description": "Llama-3.1-Nemotron-70B — это крупная языковая модель, разработанная NVIDIA, предназначенная для повышения полезности ответов, генерируемых LLM."
  },
  "o1": {
    "description": "Сосредоточена на высокоуровневом выводе и решении сложных задач, включая математические и научные задачи. Идеально подходит для приложений, требующих глубокого понимания контекста и управления рабочими процессами."
  },
  "o1-mini": {
    "description": "o1-mini — это быстрое и экономичное модель вывода, разработанная для программирования, математики и научных приложений. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "o1-preview": {
    "description": "o1 — это новая модель вывода от OpenAI, подходящая для сложных задач, требующих обширных общих знаний. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "o3-mini": {
    "description": "o3-mini — это наша последняя компактная модель вывода, обеспечивающая высокий уровень интеллекта при тех же затратах и задержках, что и o1-mini."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba — это языковая модель Mamba 2, сосредоточенная на генерации кода, обеспечивающая мощную поддержку для сложных задач по коду и выводу."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B — это компактная, но высокопроизводительная модель, хорошо подходящая для пакетной обработки и простых задач, таких как классификация и генерация текста, обладающая хорошими возможностями вывода."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo — это 12B модель, разработанная в сотрудничестве с Nvidia, обеспечивающая выдающиеся возможности вывода и кодирования, легко интегрируемая и заменяемая."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B — это более крупная экспертная модель, сосредоточенная на сложных задачах, предлагающая выдающиеся возможности вывода и более высокую пропускную способность."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B — это разреженная экспертная модель, использующая несколько параметров для повышения скорости вывода, подходит для обработки многоязычных и кодовых задач."
  },
  "openai/gpt-4o": {
    "description": "ChatGPT-4o — это динамическая модель, которая обновляется в реальном времени, чтобы оставаться актуальной. Она сочетает в себе мощные способности понимания и генерации языка, подходит для масштабных приложений, включая обслуживание клиентов, образование и техническую поддержку."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini — это последняя модель от OpenAI, выпущенная после GPT-4 Omni, поддерживающая ввод изображений и текста с выводом текста. Как их самый продвинутый компактный модель, она значительно дешевле других недавних передовых моделей и более чем на 60% дешевле GPT-3.5 Turbo. Она сохраняет передовой уровень интеллекта при значительном соотношении цена-качество. GPT-4o mini набрала 82% в тесте MMLU и в настоящее время занимает более высокое место по предпочтениям в чате, чем GPT-4."
  },
  "openai/o1-mini": {
    "description": "o1-mini — это быстрое и экономичное модель вывода, разработанная для программирования, математики и научных приложений. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "openai/o1-preview": {
    "description": "o1 — это новая модель вывода от OpenAI, подходящая для сложных задач, требующих обширных общих знаний. Модель имеет контекст 128K и срок знания до октября 2023 года."
  },
  "openchat/openchat-7b": {
    "description": "OpenChat 7B — это открытая языковая модель, оптимизированная с помощью стратегии \"C-RLFT (условное обучение с подкреплением)\"."
  },
  "openrouter/auto": {
    "description": "В зависимости от длины контекста, темы и сложности ваш запрос будет отправлен в Llama 3 70B Instruct, Claude 3.5 Sonnet (саморегулирующийся) или GPT-4o."
  },
  "phi3": {
    "description": "Phi-3 — это легковесная открытая модель, выпущенная Microsoft, подходящая для эффективной интеграции и масштабного вывода знаний."
  },
  "phi3:14b": {
    "description": "Phi-3 — это легковесная открытая модель, выпущенная Microsoft, подходящая для эффективной интеграции и масштабного вывода знаний."
  },
  "pixtral-12b-2409": {
    "description": "Модель Pixtral демонстрирует мощные способности в задачах графиков и понимания изображений, вопросов и ответов по документам, многомодального вывода и соблюдения инструкций, способная обрабатывать изображения в естественном разрешении и соотношении сторон, а также обрабатывать произвольное количество изображений в контекстном окне длиной до 128K токенов."
  },
  "pixtral-large-latest": {
    "description": "Pixtral Large — это открытая многомодальная модель с 1240 миллиардами параметров, основанная на Mistral Large 2. Это вторая модель в нашей многомодальной семье, демонстрирующая передовые уровни понимания изображений."
  },
  "pro-128k": {
    "description": "Spark Pro 128K оснащен огромной способностью обработки контекста, способной обрабатывать до 128K контекстной информации, что делает его особенно подходящим для анализа длинных текстов и обработки долгосрочных логических связей, обеспечивая плавную и последовательную логику и разнообразную поддержку ссылок в сложных текстовых коммуникациях."
  },
  "qvq-72b-preview": {
    "description": "Модель QVQ, разработанная командой Qwen, является экспериментальной исследовательской моделью, сосредоточенной на повышении визуальных способностей рассуждения, особенно в области математического рассуждения."
  },
  "qwen-coder-plus-latest": {
    "description": "Модель кода Tongyi Qianwen."
  },
  "qwen-coder-turbo-latest": {
    "description": "Модель кода Tongyi Qwen."
  },
  "qwen-long": {
    "description": "Qwen — это сверхмасштабная языковая модель, поддерживающая длинный контекст текста и диалоговые функции на основе длинных документов и нескольких документов."
  },
  "qwen-math-plus-latest": {
    "description": "Математическая модель Tongyi Qwen, специально разработанная для решения математических задач."
  },
  "qwen-math-turbo-latest": {
    "description": "Математическая модель Tongyi Qwen, специально разработанная для решения математических задач."
  },
  "qwen-max": {
    "description": "Qwen-Max — это языковая модель масштаба триллиона, поддерживающая входные данные на различных языках, включая китайский и английский. В настоящее время это API, которое стоит за продуктовой версией Qwen 2.5."
  },
  "qwen-max-latest": {
    "description": "Модель языка Tongyi Qwen с уровнем масштабирования в триллионы, поддерживающая ввод на различных языках, включая китайский и английский, является API моделью, лежащей в основе продукта Tongyi Qwen 2.5."
  },
  "qwen-plus": {
    "description": "Улучшенная версия Qwen-Turbo, поддерживающая входные данные на разных языках, включая китайский и английский."
  },
  "qwen-plus-latest": {
    "description": "Улучшенная версия модели языка Tongyi Qwen, поддерживающая ввод на различных языках, включая китайский и английский."
  },
  "qwen-turbo": {
    "description": "Qwen-Turbo — это крупная языковая модель, поддерживающая входные данные на разных языках, включая китайский и английский."
  },
  "qwen-turbo-latest": {
    "description": "Модель языка Tongyi Qwen, поддерживающая ввод на различных языках, включая китайский и английский."
  },
  "qwen-vl-chat-v1": {
    "description": "Qwen VL поддерживает гибкие способы взаимодействия, включая многократные изображения, многократные вопросы и ответы, а также творческие способности."
  },
  "qwen-vl-max-latest": {
    "description": "Супер масштабная визуально-языковая модель Tongyi Qianwen. По сравнению с улучшенной версией, еще больше повышает способности визуального вывода и соблюдения инструкций, обеспечивая более высокий уровень визуального восприятия и когнитивных способностей."
  },
  "qwen-vl-ocr-latest": {
    "description": "OCR Qwen — это специализированная модель для извлечения текста, сосредоточенная на способности извлекать текст из изображений различных типов, таких как документы, таблицы, тесты и рукописный текст. Она может распознавать множество языков, включая: китайский, английский, французский, японский, корейский, немецкий, русский, итальянский, вьетнамский и арабский."
  },
  "qwen-vl-plus-latest": {
    "description": "Улучшенная версия масштабной визуально-языковой модели Tongyi Qianwen. Значительно повышает способность распознавания деталей и текста, поддерживает разрешение более миллиона пикселей и изображения с произвольным соотношением сторон."
  },
  "qwen-vl-v1": {
    "description": "Инициализированная языковой моделью Qwen-7B, добавлена модель изображения, предобученная модель с разрешением входного изображения 448."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 — это новая серия крупных языковых моделей с более сильными возможностями понимания и генерации."
  },
  "qwen/qwen2.5-7b-instruct": {
    "description": "LLM, ориентированная на китайский и английский языки, охватывающая области языка, программирования, математики, рассуждений и др."
  },
  "qwen/qwen2.5-coder-32b-instruct": {
    "description": "Современная LLM, поддерживающая генерацию кода, рассуждения и исправления, охватывающая основные языки программирования."
  },
  "qwen/qwen2.5-coder-7b-instruct": {
    "description": "Мощная средняя модель кода, поддерживающая контекст длиной 32K, специализирующаяся на многоязычном программировании."
  },
  "qwen2": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwen2.5": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2.5-14b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 14B."
  },
  "qwen2.5-14b-instruct-1m": {
    "description": "Модель Qwen2.5 с открытым исходным кодом объемом 72B."
  },
  "qwen2.5-32b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 32B."
  },
  "qwen2.5-72b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 72B."
  },
  "qwen2.5-7b-instruct": {
    "description": "Модель Tongyi Qwen 2.5 с открытым исходным кодом объемом 7B."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Открытая версия модели кода Qwen."
  },
  "qwen2.5-coder-32b-instruct": {
    "description": "Открытая версия модели кода Tongyi Qianwen."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Открытая версия модели кода Tongyi Qwen."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "Модель Qwen-Math обладает выдающимися способностями к решению математических задач."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "Модель Qwen-Math с мощными способностями решения математических задач."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "Модель Qwen-Math с мощными способностями решения математических задач."
  },
  "qwen2.5-vl-72b-instruct": {
    "description": "Улучшение следования инструкциям, математики, решения задач и кода, улучшение способности распознавания объектов, поддержка точного позиционирования визуальных элементов в различных форматах, поддержка понимания длинных видеофайлов (максимум 10 минут) и локализация событий на уровне секунд, способность понимать последовательность времени и скорость, поддержка управления агентами ОС или мобильными устройствами на основе аналитических и позиционных возможностей, высокая способность извлечения ключевой информации и вывода в формате Json. Эта версия является 72B, самой мощной в серии."
  },
  "qwen2.5-vl-7b-instruct": {
    "description": "Улучшение следования инструкциям, математики, решения задач и кода, улучшение способности распознавания объектов, поддержка точного позиционирования визуальных элементов в различных форматах, поддержка понимания длинных видеофайлов (максимум 10 минут) и локализация событий на уровне секунд, способность понимать последовательность времени и скорость, поддержка управления агентами ОС или мобильными устройствами на основе аналитических и позиционных возможностей, высокая способность извлечения ключевой информации и вывода в формате Json. Эта версия является 72B, самой мощной в серии."
  },
  "qwen2.5:0.5b": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2.5:1.5b": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2.5:72b": {
    "description": "Qwen2.5 — это новое поколение масштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных потребностей приложений."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwen2:72b": {
    "description": "Qwen2 — это новое поколение крупномасштабной языковой модели от Alibaba, обеспечивающее отличные результаты для разнообразных приложений."
  },
  "qwq": {
    "description": "QwQ — это экспериментальная исследовательская модель, сосредоточенная на повышении возможностей вывода ИИ."
  },
  "qwq-32b-preview": {
    "description": "Модель QwQ — это экспериментальная исследовательская модель, разработанная командой Qwen, сосредоточенная на улучшении возможностей вывода ИИ."
  },
  "solar-mini": {
    "description": "Solar Mini — это компактная LLM, которая превосходит GPT-3.5, обладает мощными многоязычными возможностями, поддерживает английский и корейский языки, предлагая эффективное и компактное решение."
  },
  "solar-mini-ja": {
    "description": "Solar Mini (Ja) расширяет возможности Solar Mini, сосредотачиваясь на японском языке, при этом поддерживая высокую эффективность и выдающиеся результаты в использовании английского и корейского языков."
  },
  "solar-pro": {
    "description": "Solar Pro — это высокоинтеллектуальная LLM, выпущенная Upstage, сосредоточенная на способности следовать инструкциям на одном GPU, с оценкой IFEval выше 80. В настоящее время поддерживает английский язык, официальная версия запланирована на ноябрь 2024 года, с расширением языковой поддержки и длины контекста."
  },
  "sonar": {
    "description": "Легковесный продукт поиска на основе контекста, быстрее и дешевле, чем Sonar Pro."
  },
  "sonar-pro": {
    "description": "Расширенный продукт поиска, поддерживающий контекст поиска, сложные запросы и последующие действия."
  },
  "sonar-reasoning": {
    "description": "Новый API продукт, поддерживаемый моделью вывода DeepSeek."
  },
  "sonar-reasoning-pro": {
    "description": "Новый API продукт, поддерживаемый моделью вывода DeepSeek."
  },
  "step-1-128k": {
    "description": "Балансирует производительность и стоимость, подходит для общих сценариев."
  },
  "step-1-256k": {
    "description": "Обладает сверхдлинной способностью обработки контекста, особенно подходит для анализа длинных документов."
  },
  "step-1-32k": {
    "description": "Поддерживает диалоги средней длины, подходит для различных приложений."
  },
  "step-1-8k": {
    "description": "Маленькая модель, подходящая для легковесных задач."
  },
  "step-1-flash": {
    "description": "Высокоскоростная модель, подходящая для реального времени диалогов."
  },
  "step-1.5v-mini": {
    "description": "Эта модель обладает мощными возможностями понимания видео."
  },
  "step-1o-turbo-vision": {
    "description": "Эта модель обладает мощными способностями к пониманию изображений и превосходит 1o в области математики и кода. Модель меньше, чем 1o, и выводит результаты быстрее."
  },
  "step-1o-vision-32k": {
    "description": "Эта модель обладает мощными способностями к пониманию изображений. По сравнению с серией моделей step-1v, она имеет более высокую визуальную производительность."
  },
  "step-1v-32k": {
    "description": "Поддерживает визуальный ввод, улучшая мультимодальный опыт взаимодействия."
  },
  "step-1v-8k": {
    "description": "Небольшая визуальная модель, подходящая для базовых задач с текстом и изображениями."
  },
  "step-2-16k": {
    "description": "Поддерживает масштабные взаимодействия контекста, подходит для сложных диалоговых сценариев."
  },
  "step-2-mini": {
    "description": "Супербыстрая большая модель на основе новой самодельной архитектуры внимания MFA, достигающая аналогичных результатов, как step1, при очень низких затратах, одновременно обеспечивая более высокую пропускную способность и более быстрое время отклика. Способна обрабатывать общие задачи и обладает особыми навыками в кодировании."
  },
  "taichu_llm": {
    "description": "Модель языка TaiChu обладает выдающимися способностями к пониманию языка, а также к созданию текстов, ответам на вопросы, программированию, математическим вычислениям, логическому выводу, анализу эмоций и резюмированию текстов. Инновационно сочетает предобучение на больших данных с богатством многопоточных знаний, постоянно совершенствуя алгоритмические технологии и поглощая новые знания о словах, структуре, грамматике и семантике из огромных объемов текстовых данных, обеспечивая пользователям более удобную информацию и услуги, а также более интеллектуальный опыт."
  },
  "taichu_vl": {
    "description": "Объединяет способности к пониманию изображений, переносу знаний и логическому выводу, демонстрируя выдающиеся результаты в области вопросов и ответов на основе текста и изображений."
  },
  "text-embedding-3-large": {
    "description": "Самая мощная модель векторизации, подходящая для английских и неанглийских задач."
  },
  "text-embedding-3-small": {
    "description": "Эффективная и экономичная новая генерация модели Embedding, подходящая для поиска знаний, приложений RAG и других сценариев."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) обеспечивает повышенные вычислительные возможности благодаря эффективным стратегиям и архитектуре модели."
  },
  "tts-1": {
    "description": "Последняя модель преобразования текста в речь, оптимизированная для скорости в реальных сценариях."
  },
  "tts-1-hd": {
    "description": "Последняя модель преобразования текста в речь, оптимизированная для качества."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) подходит для детализированных командных задач, обеспечивая отличные возможности обработки языка."
  },
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0": {
    "description": "Claude 3.5 Sonnet устанавливает новые отраслевые стандарты, превосходя модели конкурентов и Claude 3 Opus, демонстрируя отличные результаты в широком спектре оценок, при этом обладая скоростью и стоимостью наших моделей среднего уровня."
  },
  "whisper-1": {
    "description": "Универсальная модель распознавания речи, поддерживающая многоязычное распознавание речи, перевод речи и распознавание языка."
  },
  "wizardlm2": {
    "description": "WizardLM 2 — это языковая модель, предоставляемая Microsoft AI, которая особенно хорошо проявляет себя в сложных диалогах, многоязычных задачах, выводе и интеллектуальных помощниках."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 — это языковая модель, предоставляемая Microsoft AI, которая особенно хорошо проявляет себя в сложных диалогах, многоязычных задачах, выводе и интеллектуальных помощниках."
  },
  "yi-large": {
    "description": "Совершенно новая модель с триллионом параметров, обеспечивающая выдающиеся возможности для вопросов и ответов, а также генерации текста."
  },
  "yi-large-fc": {
    "description": "На основе модели yi-large поддерживает и усиливает возможности вызова инструментов, подходит для различных бизнес-сценариев, требующих создания агентов или рабочих процессов."
  },
  "yi-large-preview": {
    "description": "Начальная версия, рекомендуется использовать yi-large (новую версию)."
  },
  "yi-large-rag": {
    "description": "Высококлассный сервис на основе модели yi-large, объединяющий технологии поиска и генерации для предоставления точных ответов и услуг по поиску информации в реальном времени."
  },
  "yi-large-turbo": {
    "description": "Высокая стоимость и выдающаяся производительность. Балансировка высокой точности на основе производительности, скорости вывода и затрат."
  },
  "yi-lightning": {
    "description": "Новая высокопроизводительная модель, обеспечивающая высокое качество вывода при значительно повышенной скорости вывода."
  },
  "yi-lightning-lite": {
    "description": "Упрощенная версия, рекомендуется использовать yi-lightning."
  },
  "yi-medium": {
    "description": "Модель среднего размера с улучшенной настройкой, сбалансированная по возможностям и стоимости. Глубокая оптимизация способности следовать указаниям."
  },
  "yi-medium-200k": {
    "description": "200K сверхдлинное окно контекста, обеспечивающее глубокое понимание и генерацию длинных текстов."
  },
  "yi-spark": {
    "description": "Маленькая и мощная, легковесная и быстрая модель. Обеспечивает улучшенные математические вычисления и возможности написания кода."
  },
  "yi-vision": {
    "description": "Модель для сложных визуальных задач, обеспечивающая высокую производительность в понимании и анализе изображений."
  },
  "yi-vision-v2": {
    "description": "Модель для сложных визуальных задач, обеспечивающая высокопроизводительное понимание и анализ на основе нескольких изображений."
  }
}
